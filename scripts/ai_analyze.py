#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI åˆ†æè„šæœ¬ï¼ˆåŸºäºæ•°æ®åº“ï¼‰

åŠŸèƒ½ï¼š
- ä» `data/news_data.db` è¯»å–æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„æ–‡ç« 
- è¯­æ–™æ„é€ ä¼˜å…ˆä½¿ç”¨ `content`ï¼ˆæ­£æ–‡ï¼‰ï¼Œä¸ºç©ºåˆ™å›é€€ `summary`
- è°ƒç”¨ Gemini æ¨¡å‹ç”Ÿæˆ Markdown åˆ†æï¼ˆå¤šæ¨¡å‹å…œåº•ï¼‰
- å°†æŠ¥å‘Šä¿å­˜åˆ° `docs/archive/YYYY-MM/YYYY-MM-DD/reports/` ä¸‹
- å¯é€‰å¯¼å‡º JSONï¼ˆåŒ…å« summary ä¸æ–‡ç« å…ƒæ•°æ®ï¼‰

ç¤ºä¾‹ï¼š
  - åˆ†æå½“å¤©ï¼š
      python3 scripts/ai_analyze.py
  - æŒ‡å®šæ—¥æœŸï¼š
      python3 scripts/ai_analyze.py --date 2025-09-29
  - æŒ‡å®šèŒƒå›´å¹¶å¯¼å‡º JSONï¼š
      python3 scripts/ai_analyze.py --start 2025-09-28 --end 2025-09-29 --output-json /tmp/analysis.json
"""

import argparse
import json
import os
import sqlite3
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple

import pytz
import yaml

try:
    import google.generativeai as genai
except Exception:  # å…è®¸ç¯å¢ƒç¼ºå¤±æ—¶å…ˆè¡Œæç¤º
    genai = None


PROJECT_ROOT = Path(__file__).resolve().parents[1]
DB_PATH = PROJECT_ROOT / 'data' / 'news_data.db'


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description='ä»æ•°æ®åº“è¯»å–æ–°é—»å¹¶è°ƒç”¨ Gemini ç”Ÿæˆåˆ†ææŠ¥å‘Š')
    date_group = parser.add_mutually_exclusive_group()
    date_group.add_argument('--date', type=str, help='æŒ‡å®šå•æ—¥ï¼ˆYYYY-MM-DDï¼‰')
    parser.add_argument('--start', type=str, help='å¼€å§‹æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰ï¼Œé»˜è®¤ä¸ºå½“å¤©')
    parser.add_argument('--end', type=str, help='ç»“æŸæ—¥æœŸï¼ˆYYYY-MM-DDï¼‰ï¼Œé»˜è®¤ä¸ºå½“å¤©')
    parser.add_argument('--limit', type=int, default=0, help='æœ€å¤šè¯»å–å¤šå°‘æ¡è®°å½•ï¼ˆ0è¡¨ç¤ºä¸é™åˆ¶ï¼‰')
    parser.add_argument('--max-articles', type=int, help='å¯é€‰ï¼šå¯¹å‚ä¸åˆ†æçš„æ–‡ç« å†æ§é‡ï¼ˆä¼˜å…ˆçº§é«˜äº --limitï¼‰')
    parser.add_argument('--filter-source', type=str, help='ä»…åˆ†ææŒ‡å®šæ¥æºï¼ˆé€—å·åˆ†éš”ï¼‰')
    parser.add_argument('--filter-keyword', type=str, help='ä»…åˆ†ææ ‡é¢˜/æ‘˜è¦åŒ…å«å…³é”®è¯çš„æ–‡ç« ï¼ˆé€—å·åˆ†éš”ï¼ŒORè¯­ä¹‰ï¼‰')
    parser.add_argument('--order', choices=['asc', 'desc'], default='desc', help='æ’åºæ–¹å‘ï¼ŒåŸºäº published ä¼˜å…ˆã€å¦åˆ™ created_at')
    parser.add_argument('--output-json', type=str, help='å¯é€‰ï¼šå°†ç»“æœï¼ˆsummary+æ–‡ç« å…ƒæ•°æ®ï¼‰å¯¼å‡ºä¸º JSON æ–‡ä»¶')
    parser.add_argument('--max-chars', type=int, default=200000, help='ä¼ å…¥æ¨¡å‹çš„æœ€å¤§å­—ç¬¦æ•°ä¸Šé™ï¼Œç”¨äºæ§åˆ¶æˆæœ¬ï¼Œ0 è¡¨ç¤ºä¸é™åˆ¶')
    parser.add_argument('--api-key', type=str, help='å¯é€‰ï¼šæ˜¾å¼ä¼ å…¥ Gemini API Keyï¼ˆé»˜è®¤ä»é…ç½®/ç¯å¢ƒè¯»å–ï¼‰')
    parser.add_argument('--config', type=str, help='å¯é€‰ï¼šé…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ config/config.ymlï¼‰')
    return parser.parse_args()


def validate_date(date_str: str) -> str:
    try:
        datetime.strptime(date_str, '%Y-%m-%d')
        return date_str
    except ValueError:
        raise SystemExit(f'æ— æ•ˆæ—¥æœŸæ ¼å¼: {date_str}ï¼Œåº”ä¸º YYYY-MM-DD')


def resolve_date_range(args: argparse.Namespace) -> Tuple[str, str]:
    today = datetime.now(pytz.timezone('Asia/Shanghai')).strftime('%Y-%m-%d')
    if args.date:
        day = validate_date(args.date)
        return day, day
    start = validate_date(args.start) if args.start else today
    end = validate_date(args.end) if args.end else today
    if start > end:
        raise SystemExit(f'å¼€å§‹æ—¥æœŸä¸å¾—æ™šäºç»“æŸæ—¥æœŸ: {start} > {end}')
    return start, end


def open_connection(db_path: Path) -> sqlite3.Connection:
    if not db_path.exists():
        raise SystemExit(f'æ•°æ®åº“ä¸å­˜åœ¨: {db_path}')
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    return conn


def build_query(order: str, limit: int) -> Tuple[str, List[Any]]:
    sql = [
        'SELECT a.id, a.collection_date, a.title, a.link, a.published, a.summary, a.content, s.source_name',
        'FROM news_articles a',
        'JOIN rss_sources s ON a.source_id = s.id',
        'WHERE a.collection_date BETWEEN ? AND ?'
    ]
    params: List[Any] = []

    order_dir = 'DESC' if order.lower() == 'desc' else 'ASC'
    sql.append('ORDER BY COALESCE(a.published, a.created_at) ' + order_dir)

    if limit and limit > 0:
        sql.append('LIMIT ?')
        params.append(limit)

    return '\n'.join(sql), params


def query_articles(conn: sqlite3.Connection, start: str, end: str, order: str, limit: int) -> List[Dict[str, Any]]:
    sql, tail = build_query(order, limit)
    params = [start, end] + tail
    cur = conn.execute(sql, params)
    rows = cur.fetchall()
    results: List[Dict[str, Any]] = []
    for r in rows:
        results.append({
            'id': r['id'],
            'collection_date': r['collection_date'],
            'title': r['title'],
            'link': r['link'],
            'source': r['source_name'],
            'published': r['published'],
            'summary': r['summary'],
            'content': r['content']
        })
    return results


def chunk_text(text: str, max_chars: int = 4000) -> List[str]:
    """ç®€å•å­—ç¬¦çº§åˆ†å—ï¼ˆè¿‘ä¼¼ 500-1500 tokensï¼‰ã€‚å¯æ›¿æ¢æˆæ›´æ™ºèƒ½çš„è¯­ä¹‰åˆ‡åˆ†ã€‚"""
    if not text:
        return []
    if max_chars <= 0:
        return [text]
    chunks: List[str] = []
    start = 0
    n = len(text)
    while start < n:
        end = min(n, start + max_chars)
        # å°è¯•åœ¨æ®µè½è¾¹ç•Œæˆªæ–­
        boundary = text.rfind('\n\n', start, end)
        if boundary == -1 or boundary <= start + int(max_chars * 0.5):
            boundary = end
        chunks.append(text[start:boundary])
        start = boundary
    return chunks


def build_corpus(articles: List[Dict[str, Any]], max_chars: int, per_chunk_chars: int = 3000) -> Tuple[List[Tuple[Dict[str, Any], List[str]]], int]:
    """æ„é€ åˆ†å—è¯­æ–™ï¼šè¿”å› [(article_meta, [chunks...])...] ä¸åŸå§‹æ€»é•¿åº¦ã€‚"""
    pairs: List[Tuple[Dict[str, Any], List[str]]] = []
    total_len = 0
    for a in articles:
        body = a.get('content') or a.get('summary') or ''
        title = a.get('title') or ''
        source = a.get('source') or ''
        published = a.get('published') or ''
        link = a.get('link') or ''
        header = f"ã€{title}ã€‘\næ¥æº: {source} | æ—¶é—´: {published}\né“¾æ¥: {link}\n"
        text = header + body
        total_len += len(text)
        chunks = chunk_text(text, per_chunk_chars)
        pairs.append((a, chunks))

    # ä¸Šé™æ§åˆ¶ï¼ˆç²—ç•¥æŒ‰å­—ç¬¦è£å‰ªï¼‰ï¼šä»…åœ¨ max_chars > 0 æ—¶ç”Ÿæ•ˆ
    if max_chars and max_chars > 0:
        acc = 0
        trimmed: List[Tuple[Dict[str, Any], List[str]]] = []
        for meta, chunks in pairs:
            kept: List[str] = []
            for c in chunks:
                if acc + len(c) <= max_chars:
                    kept.append(c)
                    acc += len(c)
                else:
                    break
            if kept:
                trimmed.append((meta, kept))
            if acc >= max_chars:
                break
        return trimmed, total_len
    return pairs, total_len


def call_gemini(api_key: str, content: str) -> Tuple[str, Dict[str, Any]]:
    """æŒ‰ä¼˜å…ˆçº§å°è¯•å¤šä¸ªæ¨¡å‹ï¼Œè¿”å› Markdown æ–‡æœ¬ã€‚"""
    if genai is None:
        raise SystemExit('æœªå®‰è£… google-generativeaiï¼Œè¯·å…ˆå®‰è£…æˆ–åœ¨ç¯å¢ƒä¸­æä¾›ã€‚')

    model_names = [
        'models/gemini-2.5-pro',
        'models/gemini-2.5-flash',
        'models/gemini-2.0-flash',
        'models/gemini-pro-latest'
    ]

    genai.configure(api_key=api_key)
    print(f'ğŸ¤– æ­£åœ¨ç”ŸæˆæŠ¥å‘Šï¼ˆè¾“å…¥é•¿åº¦ {len(content)} å­—ç¬¦ï¼‰')

    # è¯»å–æç¤ºè¯ï¼ˆå›ºå®šä½¿ç”¨ä¸“ä¸šç‰ˆï¼‰
    prompt_path = PROJECT_ROOT / 'task' / 'financial_analysis_prompt_pro.md'
    if not prompt_path.exists():
        raise SystemExit(f'æç¤ºè¯æ–‡ä»¶ä¸å­˜åœ¨: {prompt_path}')
    with open(prompt_path, 'r', encoding='utf-8') as f:
        system_prompt = f.read()

    last_error: Optional[Exception] = None
    for model_name in model_names:
        try:
            print(f'â†’ å°è¯•æ¨¡å‹: {model_name}')
            model = genai.GenerativeModel(model_name)
            resp = model.generate_content([system_prompt, content])
            print(f'âœ… æ¨¡å‹æˆåŠŸ: {model_name}')
            usage = {}
            try:
                usage = {
                    'model': model_name,
                    'prompt_tokens': getattr(resp, 'usage_metadata', {}).get('prompt_token_count'),
                    'candidates_tokens': getattr(resp, 'usage_metadata', {}).get('candidates_token_count'),
                    'total_tokens': getattr(resp, 'usage_metadata', {}).get('total_token_count')
                }
            except Exception:
                pass
            return resp.text, usage
        except Exception as e:  # å°è¯•ä¸‹ä¸€ä¸ª
            last_error = e
            continue

    raise RuntimeError(f'æ‰€æœ‰æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œæœ€åé”™è¯¯ï¼š{last_error}')


def save_markdown(date_str: str, markdown_text: str) -> Path:
    year_month = date_str[:7]
    report_dir = PROJECT_ROOT / 'docs' / 'archive' / year_month / date_str / 'reports'
    report_dir.mkdir(parents=True, exist_ok=True)
    now_str = datetime.now(pytz.timezone('Asia/Shanghai')).strftime('%Y-%m-%d %H:%M:%S')
    header = f"# ğŸ“… {date_str} è´¢ç»åˆ†ææŠ¥å‘Š\n\n> ğŸ“… ç”Ÿæˆæ—¶é—´: {now_str} (åŒ—äº¬æ—¶é—´)\n\n"
    content = header + (markdown_text or '').strip() + '\n'
    report_file = report_dir / f"ğŸ“… {date_str} è´¢ç»åˆ†ææŠ¥å‘Š.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(content)
    print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    return report_file


def save_metadata(date_str: str, meta: Dict[str, Any]):
    year_month = date_str[:7]
    report_dir = PROJECT_ROOT / 'docs' / 'archive' / year_month / date_str / 'reports'
    report_dir.mkdir(parents=True, exist_ok=True)
    meta_file = report_dir / 'analysis_meta.json'
    with open(meta_file, 'w', encoding='utf-8') as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)
    print(f'ğŸ§¾ å…ƒæ•°æ®å·²ä¿å­˜åˆ°: {meta_file}')


def write_json(path: Path, summary_md: str, articles: List[Dict[str, Any]]):
    data = {
        'summary_markdown': summary_md,
        'articles': articles
    }
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f'âœ… å·²å¯¼å‡º JSON: {path}')


def main():
    args = parse_args()
    start, end = resolve_date_range(args)
    print(f'ğŸš€ å¼€å§‹ AI åˆ†æï¼šæ—¥æœŸèŒƒå›´ {start} â†’ {end}')

    # è§£æé…ç½®æ–‡ä»¶ï¼Œä¼˜å…ˆé¡ºåºï¼šconfig.yml > --api-key > ç¯å¢ƒå˜é‡
    config_path = Path(args.config) if args.config else (PROJECT_ROOT / 'config' / 'config.yml')
    api_key: Optional[str] = None
    if config_path.exists():
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                cfg = yaml.safe_load(f) or {}
            # å¸¸è§å±‚çº§å…¼å®¹ï¼šapi_keys.gemini æˆ– gemini.api_key
            api_key = (
                (cfg.get('api_keys') or {}).get('gemini')
                or (cfg.get('gemini') or {}).get('api_key')
            )
            print(f'ğŸ”§ ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼š{config_path}')
        except Exception as e:
            print(f'âš ï¸ è¯»å–é…ç½®å¤±è´¥ï¼ˆ{config_path}ï¼‰ï¼š{e}ï¼Œå°†å°è¯•ä½¿ç”¨å‘½ä»¤è¡Œæˆ–ç¯å¢ƒå˜é‡ã€‚')
    else:
        print(f'âš ï¸ æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼š{config_path}ï¼Œå°†å°è¯•ä½¿ç”¨å‘½ä»¤è¡Œæˆ–ç¯å¢ƒå˜é‡ã€‚')

    if not api_key:
        api_key = args.api_key or os.getenv('GEMINI_API_KEY')
    if not api_key:
        raise SystemExit("æœªåœ¨é…ç½®/å‚æ•°/ç¯å¢ƒä¸­æ‰¾åˆ° Gemini API Keyã€‚å¯åœ¨ config.yml çš„ api_keys.gemini æˆ– gemini.api_key é…ç½®ï¼Œæˆ–ä½¿ç”¨ --api-key / GEMINI_API_KEYã€‚")

    conn = open_connection(DB_PATH)
    try:
        rows = query_articles(conn, start, end, args.order, args.limit)
    finally:
        conn.close()

    if not rows:
        print('ï¼ˆæ— ç»“æœï¼‰æœªæ‰¾åˆ°æŒ‡å®šæ—¥æœŸèŒƒå›´çš„æ–‡ç« ï¼Œç»ˆæ­¢åˆ†æã€‚')
        return
    print(f'ğŸ“¥ å·²è¯»å–æ–‡ç« ï¼š{len(rows)} æ¡')

    # è¿‡æ»¤æ¥æºä¸å…³é”®è¯ï¼ˆå¯æ§é‡ï¼‰
    selected = rows
    if args.filter_source:
        sources = {s.strip() for s in args.filter_source.split(',') if s.strip()}
        selected = [r for r in selected if (r.get('source') or '') in sources]
    if args.filter_keyword:
        kws = {k.strip() for k in args.filter_keyword.split(',') if k.strip()}
        def match_kw(r: Dict[str, Any]) -> bool:
            text = f"{r.get('title','')} {r.get('summary','')}".lower()
            return any(k.lower() in text for k in kws)
        selected = [r for r in selected if match_kw(r)]
    if args.max_articles and args.max_articles > 0:
        selected = selected[:args.max_articles]

    pairs, total_len = build_corpus(selected, args.max_chars, per_chunk_chars=3000)
    current_len = sum(len(c) for _, chunks in pairs for c in chunks)
    print(f'ğŸ” è¯­æ–™é•¿åº¦: {current_len} å­—ç¬¦ï¼ˆåŸå§‹ {total_len}ï¼Œmax={args.max_chars}ï¼‰')
    if args.max_chars and args.max_chars > 0 and total_len > args.max_chars:
        print(f'âœ‚ï¸ è¯­æ–™å·²æŒ‰ä¸Šé™æˆªæ–­ï¼š{total_len} â†’ {current_len}')

    # ç®€å• RAGï¼šå°†åˆ†å—ä¸²æ¥ä¸€æ¬¡æ€§ç”Ÿæˆï¼ˆå¯å‡çº§ä¸ºå…ˆå¬å›å†ç”Ÿæˆï¼‰
    joined = '\n\n'.join(c for _, chunks in pairs for c in chunks)
    try:
        summary_md, usage = call_gemini(api_key, joined)
    except Exception as e:
        print(f'âŒ æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}')
        return

    # ä¿å­˜ Markdown æŠ¥å‘Šï¼ˆæŒ‰ end æ—¥æœŸå‘½åï¼Œæ›´è´´è¿‘æ—¥æŠ¥è¯­ä¹‰ï¼‰
    saved_path = save_markdown(end, summary_md)
    # å…ƒæ•°æ®æŒä¹…åŒ–
    meta = {
        'date_range': {'start': start, 'end': end},
        'articles_used': len(selected),
        'chunks': sum(len(ch) for _, ch in pairs),
        'model_usage': usage,
#         'prompt_file': str((PROJECT_ROOT / 'task' / 'financial_analysis_prompt_pro.md').resolve())
    }
    save_metadata(end, meta)

    # å¯é€‰å¯¼å‡º JSON
    if args.output_json:
        out_path = Path(args.output_json)
        if not out_path.is_absolute():
            out_path = PROJECT_ROOT / out_path
        write_json(out_path, summary_md, rows)

    print('ğŸ‰ åˆ†æå®Œæˆã€‚')


if __name__ == '__main__':
    main()


