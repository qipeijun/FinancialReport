#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI åˆ†æè„šæœ¬ï¼ˆåŸºäºæ•°æ®åº“ï¼‰

åŠŸèƒ½ï¼š
- ä» `data/news_data.db` è¯»å–æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„æ–‡ç« 
- è¯­æ–™æ„é€ ä¼˜å…ˆä½¿ç”¨ `content`ï¼ˆæ­£æ–‡ï¼‰ï¼Œä¸ºç©ºåˆ™å›é€€ `summary`
- è°ƒç”¨ Gemini æ¨¡å‹ç”Ÿæˆ Markdown åˆ†æï¼ˆå¤šæ¨¡å‹å…œåº•ï¼‰
- å°†æŠ¥å‘Šä¿å­˜åˆ° `docs/archive/YYYY-MM/YYYY-MM-DD/reports/` ä¸‹
- å¯é€‰å¯¼å‡º JSONï¼ˆåŒ…å« summary ä¸æ–‡ç« å…ƒæ•°æ®ï¼‰

ç¤ºä¾‹ï¼š
  - åˆ†æå½“å¤©ï¼š
      python3 scripts/ai_analyze.py
  - æŒ‡å®šæ—¥æœŸï¼š
      python3 scripts/ai_analyze.py --date 2025-09-29
  - æŒ‡å®šèŒƒå›´å¹¶å¯¼å‡º JSONï¼š
      python3 scripts/ai_analyze.py --start 2025-09-28 --end 2025-09-29 --output-json /tmp/analysis.json
"""

import argparse
import json
import os
import sqlite3
import sys
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple

import pytz
import yaml

from utils.print_utils import (
    print_header, print_success, print_warning, print_error,
    print_info, print_progress, print_step, print_statistics
)

try:
    import google.generativeai as genai
except Exception:  # å…è®¸ç¯å¢ƒç¼ºå¤±æ—¶å…ˆè¡Œæç¤º
    genai = None


PROJECT_ROOT = Path(__file__).resolve().parents[1]
DB_PATH = PROJECT_ROOT / 'data' / 'news_data.db'


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description='ä»æ•°æ®åº“è¯»å–æ–°é—»å¹¶è°ƒç”¨ Gemini ç”Ÿæˆåˆ†ææŠ¥å‘Š')
    date_group = parser.add_mutually_exclusive_group()
    date_group.add_argument('--date', type=str, help='æŒ‡å®šå•æ—¥ï¼ˆYYYY-MM-DDï¼‰')
    parser.add_argument('--start', type=str, help='å¼€å§‹æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰ï¼Œé»˜è®¤ä¸ºå½“å¤©')
    parser.add_argument('--end', type=str, help='ç»“æŸæ—¥æœŸï¼ˆYYYY-MM-DDï¼‰ï¼Œé»˜è®¤ä¸ºå½“å¤©')
    parser.add_argument('--limit', type=int, default=0, help='æœ€å¤šè¯»å–å¤šå°‘æ¡è®°å½•ï¼ˆ0è¡¨ç¤ºä¸é™åˆ¶ï¼‰')
    parser.add_argument('--max-articles', type=int, help='å¯é€‰ï¼šå¯¹å‚ä¸åˆ†æçš„æ–‡ç« å†æ§é‡ï¼ˆä¼˜å…ˆçº§é«˜äº --limitï¼‰')
    parser.add_argument('--filter-source', type=str, help='ä»…åˆ†ææŒ‡å®šæ¥æºï¼ˆé€—å·åˆ†éš”ï¼‰')
    parser.add_argument('--filter-keyword', type=str, help='ä»…åˆ†ææ ‡é¢˜/æ‘˜è¦åŒ…å«å…³é”®è¯çš„æ–‡ç« ï¼ˆé€—å·åˆ†éš”ï¼ŒORè¯­ä¹‰ï¼‰')
    parser.add_argument('--order', choices=['asc', 'desc'], default='desc', help='æ’åºæ–¹å‘ï¼ŒåŸºäº published ä¼˜å…ˆã€å¦åˆ™ created_at')
    parser.add_argument('--output-json', type=str, help='å¯é€‰ï¼šå°†ç»“æœï¼ˆsummary+æ–‡ç« å…ƒæ•°æ®ï¼‰å¯¼å‡ºä¸º JSON æ–‡ä»¶')
    parser.add_argument('--max-chars', type=int, default=500000, help='ä¼ å…¥æ¨¡å‹çš„æœ€å¤§å­—ç¬¦æ•°ä¸Šé™ï¼Œç”¨äºæ§åˆ¶æˆæœ¬ï¼Œ0 è¡¨ç¤ºä¸é™åˆ¶')
    parser.add_argument('--api-key', type=str, help='å¯é€‰ï¼šæ˜¾å¼ä¼ å…¥ Gemini API Keyï¼ˆé»˜è®¤ä»é…ç½®/ç¯å¢ƒè¯»å–ï¼‰')
    parser.add_argument('--config', type=str, help='å¯é€‰ï¼šé…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ config/config.ymlï¼‰')
    parser.add_argument('--content-field', choices=['summary', 'content', 'auto'], default='summary', help='é€‰æ‹©åˆ†æå­—æ®µï¼šsummary(æ‘˜è¦ä¼˜å…ˆï¼Œé»˜è®¤)ã€content(æ­£æ–‡ä¼˜å…ˆ)ã€auto(æ™ºèƒ½é€‰æ‹©)')
    parser.add_argument('--model', type=str, help='å¯é€‰ï¼šæŒ‡å®š Gemini æ¨¡å‹ï¼ˆå¦‚ gemini-2.5-proï¼‰ï¼Œä¸æŒ‡å®šåˆ™æŒ‰ä¼˜å…ˆçº§è‡ªåŠ¨é€‰æ‹©')
    return parser.parse_args()


def validate_date(date_str: str) -> str:
    try:
        datetime.strptime(date_str, '%Y-%m-%d')
        return date_str
    except ValueError:
        raise SystemExit(f'æ— æ•ˆæ—¥æœŸæ ¼å¼: {date_str}ï¼Œåº”ä¸º YYYY-MM-DD')


def resolve_date_range(args: argparse.Namespace) -> Tuple[str, str]:
    today = datetime.now(pytz.timezone('Asia/Shanghai')).strftime('%Y-%m-%d')
    if args.date:
        day = validate_date(args.date)
        return day, day
    start = validate_date(args.start) if args.start else today
    end = validate_date(args.end) if args.end else today
    if start > end:
        raise SystemExit(f'å¼€å§‹æ—¥æœŸä¸å¾—æ™šäºç»“æŸæ—¥æœŸ: {start} > {end}')
    return start, end


def open_connection(db_path: Path) -> sqlite3.Connection:
    if not db_path.exists():
        raise SystemExit(f'æ•°æ®åº“ä¸å­˜åœ¨: {db_path}')
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    return conn


def build_query(order: str, limit: int) -> Tuple[str, List[Any]]:
    sql = [
        'SELECT a.id, a.collection_date, a.title, a.link, a.published, a.summary, a.content, s.source_name',
        'FROM news_articles a',
        'JOIN rss_sources s ON a.source_id = s.id',
        'WHERE a.collection_date BETWEEN ? AND ?'
    ]
    params: List[Any] = []

    order_dir = 'DESC' if order.lower() == 'desc' else 'ASC'
    sql.append('ORDER BY COALESCE(a.published, a.created_at) ' + order_dir)

    if limit and limit > 0:
        sql.append('LIMIT ?')
        params.append(limit)

    return '\n'.join(sql), params


def query_articles(conn: sqlite3.Connection, start: str, end: str, order: str, limit: int) -> List[Dict[str, Any]]:
    sql, tail = build_query(order, limit)
    params = [start, end] + tail
    cur = conn.execute(sql, params)
    rows = cur.fetchall()
    results: List[Dict[str, Any]] = []
    for r in rows:
        results.append({
            'id': r['id'],
            'collection_date': r['collection_date'],
            'title': r['title'],
            'link': r['link'],
            'source': r['source_name'],
            'published': r['published'],
            'summary': r['summary'],
            'content': r['content']
        })
    return results


def chunk_text(text: str, max_chars: int = 4000) -> List[str]:
    """ç®€å•å­—ç¬¦çº§åˆ†å—ï¼ˆè¿‘ä¼¼ 500-1500 tokensï¼‰ã€‚å¯æ›¿æ¢æˆæ›´æ™ºèƒ½çš„è¯­ä¹‰åˆ‡åˆ†ã€‚"""
    if not text:
        return []
    if max_chars <= 0:
        return [text]
    chunks: List[str] = []
    start = 0
    n = len(text)
    while start < n:
        end = min(n, start + max_chars)
        # å°è¯•åœ¨æ®µè½è¾¹ç•Œæˆªæ–­
        boundary = text.rfind('\n\n', start, end)
        if boundary == -1 or boundary <= start + int(max_chars * 0.5):
            boundary = end
        chunks.append(text[start:boundary])
        start = boundary
    return chunks


def build_corpus(articles: List[Dict[str, Any]], max_chars: int, per_chunk_chars: int = 3000, content_field: str = 'auto') -> Tuple[List[Tuple[Dict[str, Any], List[str]]], int]:
    """æ„é€ åˆ†å—è¯­æ–™ï¼šè¿”å› [(article_meta, [chunks...])...] ä¸åŸå§‹æ€»é•¿åº¦ã€‚"""
    pairs: List[Tuple[Dict[str, Any], List[str]]] = []
    total_len = 0
    for a in articles:
        if content_field == 'summary':
            body = a.get('summary') or a.get('content') or ''
        elif content_field == 'content':
            body = a.get('content') or a.get('summary') or ''
        else:  # auto - æ™ºèƒ½é€‰æ‹©
            summary = a.get('summary', '')
            content = a.get('content', '')
            # å¦‚æœcontentå¤ªé•¿ï¼ˆ>5000å­—ç¬¦ï¼‰ï¼Œä¼˜å…ˆä½¿ç”¨summary
            if len(content) > 5000 and summary:
                body = summary
            else:
                body = content or summary or ''
        title = a.get('title') or ''
        source = a.get('source') or ''
        published = a.get('published') or ''
        link = a.get('link') or ''
        header = f"ã€{title}ã€‘\næ¥æº: {source} | æ—¶é—´: {published}\né“¾æ¥: {link}\n"
        text = header + body
        total_len += len(text)
        chunks = chunk_text(text, per_chunk_chars)
        pairs.append((a, chunks))

    # ä¸Šé™æ§åˆ¶ï¼ˆç²—ç•¥æŒ‰å­—ç¬¦è£å‰ªï¼‰ï¼šä»…åœ¨ max_chars > 0 æ—¶ç”Ÿæ•ˆ
    if max_chars and max_chars > 0:
        acc = 0
        trimmed: List[Tuple[Dict[str, Any], List[str]]] = []
        for meta, chunks in pairs:
            kept: List[str] = []
            for c in chunks:
                if acc + len(c) <= max_chars:
                    kept.append(c)
                    acc += len(c)
                else:
                    break
            if kept:
                trimmed.append((meta, kept))
            if acc >= max_chars:
                break
        return trimmed, total_len
    return pairs, total_len


def _normalize_source_name(name: str) -> str:
    if not name:
        return 'æœªçŸ¥æ¥æº'
    name = name.strip()
    mapping = {
        'ä¸œæ–¹è´¢å¯Œç½‘': 'ä¸œæ–¹è´¢å¯Œ',
        'å›½å®¶ç»Ÿè®¡å±€-æœ€æ–°å‘å¸ƒ': 'å›½å®¶ç»Ÿè®¡å±€',
        'ä¸­æ–°ç¤¾': 'ä¸­æ–°ç½‘',
        'ä¸­å›½æ–°é—»ç½‘': 'ä¸­æ–°ç½‘',
        'Wall Street CN': 'åå°”è¡—è§é—»',
        'WallstreetCN': 'åå°”è¡—è§é—»',
    }
    return mapping.get(name, name)


def build_source_stats_block(selected: List[Dict[str, Any]], content_field: str, start: str, end: str) -> str:
    tracked = ['åå°”è¡—è§é—»', '36æ°ª', 'ä¸œæ–¹è´¢å¯Œ', 'å›½å®¶ç»Ÿè®¡å±€', 'ä¸­æ–°ç½‘']
    counters: Dict[str, int] = {k: 0 for k in tracked}
    other_count = 0

    for article in selected:
        raw = (article.get('source') or '').strip()
        norm = _normalize_source_name(raw)
        if norm in counters:
            counters[norm] += 1
        else:
            other_count += 1

    total_articles = len(selected)
    content_articles = sum(1 for a in selected if a.get('content'))
    content_ratio = (content_articles / total_articles * 100) if total_articles > 0 else 0

    stats_info = f"""
=== æ•°æ®ç»Ÿè®¡ä¿¡æ¯ ===
åˆ†ææ—¥æœŸèŒƒå›´: {start} è‡³ {end}
å¤„ç†æ–‡ç« æ€»æ•°: {total_articles}ç¯‡
å†…å®¹ç±»å‹: {content_field}
æ•°æ®å®Œæ•´æ€§: {content_ratio:.1f}%çš„æ–‡ç« åŒ…å«å®Œæ•´å†…å®¹

æ–°é—»æºç»Ÿè®¡:
æœ¬æ¬¡åˆ†æåŸºäºä»¥ä¸‹æ–°é—»æºï¼š
"""
    for k in tracked:
        stats_info += f"- {k}ï¼š{counters[k]}ç¯‡\n"
    stats_info += f"- å…¶ä»–æ¥æºï¼š{other_count}ç¯‡\n\n"
    stats_info += f"æ€»è®¡: {total_articles}ç¯‡æ–°é—»æ–‡ç« \n"
    return stats_info

def call_gemini(api_key: str, content: str, preferred_model: Optional[str] = None) -> Tuple[str, Dict[str, Any]]:
    """æŒ‰ä¼˜å…ˆçº§å°è¯•å¤šä¸ªæ¨¡å‹ï¼Œè¿”å› Markdown æ–‡æœ¬ã€‚"""
    if genai is None:
        raise SystemExit('æœªå®‰è£… google-generativeaiï¼Œè¯·å…ˆå®‰è£…æˆ–åœ¨ç¯å¢ƒä¸­æä¾›ã€‚')

    # å¦‚æœæŒ‡å®šäº†æ¨¡å‹ï¼Œåªå°è¯•è¯¥æ¨¡å‹
    if preferred_model:
        model_names = [f'models/{preferred_model}' if not preferred_model.startswith('models/') else preferred_model]
        print_info(f'ä½¿ç”¨æŒ‡å®šæ¨¡å‹: {model_names[0]}')
    else:
        # é»˜è®¤æŒ‰ä¼˜å…ˆçº§å°è¯•
        model_names = [
            'models/gemini-2.5-pro',
            'models/gemini-2.5-flash',
            'models/gemini-2.0-flash',
            'models/gemini-pro-latest'
        ]
        print_info('æŒ‰ä¼˜å…ˆçº§å°è¯•æ¨¡å‹: 2.5-pro â†’ 2.5-flash â†’ 2.0-flash â†’ pro-latest')

    genai.configure(api_key=api_key)
    print_progress(f'æ­£åœ¨ç”ŸæˆæŠ¥å‘Šï¼ˆè¾“å…¥é•¿åº¦ {len(content):,} å­—ç¬¦ï¼‰')

    # è¯»å–æç¤ºè¯ï¼ˆå›ºå®šä½¿ç”¨ä¸“ä¸šç‰ˆï¼‰
    prompt_path = PROJECT_ROOT / 'task' / 'financial_analysis_prompt_pro.md'
    if not prompt_path.exists():
        raise SystemExit(f'æç¤ºè¯æ–‡ä»¶ä¸å­˜åœ¨: {prompt_path}')
    with open(prompt_path, 'r', encoding='utf-8') as f:
        system_prompt = f.read()
    
    # æ›¿æ¢æ¨¡å‹å ä½ç¬¦ï¼ˆåœ¨å®é™…è°ƒç”¨å‰ä¼šè¢«æ›¿æ¢ä¸ºçœŸå®æ¨¡å‹åï¼‰
    # æ³¨æ„ï¼šè¿™é‡Œå…ˆç”¨å ä½ç¬¦ï¼ŒæˆåŠŸè°ƒç”¨åå†æ›¿æ¢
    system_prompt_template = system_prompt

    last_error: Optional[Exception] = None
    for i, model_name in enumerate(model_names, 1):
        try:
            print_step(i, len(model_names), f'å°è¯•æ¨¡å‹: {model_name}')
            
            # ä¸ºæ¯ä¸ªæ¨¡å‹æ›¿æ¢å ä½ç¬¦
            system_prompt = system_prompt_template.replace('[ä½¿ç”¨çš„å…·ä½“æ¨¡å‹åç§°]', model_name.replace('models/', ''))
            
            model = genai.GenerativeModel(model_name)
            resp = model.generate_content([system_prompt, content])
            print_success(f'æ¨¡å‹è°ƒç”¨æˆåŠŸ: {model_name}')
            usage = {'model': model_name}
            try:
                if hasattr(resp, 'usage_metadata') and resp.usage_metadata:
                    usage_metadata = resp.usage_metadata
                    usage['prompt_tokens'] = getattr(usage_metadata, 'prompt_token_count', 0)
                    usage['candidates_tokens'] = getattr(usage_metadata, 'candidates_token_count', 0)
                    usage['total_tokens'] = getattr(usage_metadata, 'total_token_count', 0)
            except Exception:
                pass  # é™é»˜å¤±è´¥ï¼Œè‡³å°‘ä¿è¯modelå­—æ®µå­˜åœ¨
            return resp.text, usage
        except Exception as e:  # å°è¯•ä¸‹ä¸€ä¸ª
            last_error = e
            continue

    raise RuntimeError(f'æ‰€æœ‰æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œæœ€åé”™è¯¯ï¼š{last_error}')


def save_markdown(date_str: str, markdown_text: str) -> Path:
    year_month = date_str[:7]
    report_dir = PROJECT_ROOT / 'docs' / 'archive' / year_month / date_str / 'reports'
    report_dir.mkdir(parents=True, exist_ok=True)
    now = datetime.now(pytz.timezone('Asia/Shanghai'))
    now_str = now.strftime('%Y-%m-%d %H:%M:%S')
    # æ ¹æ®å°æ—¶åˆ¤æ–­ AM/PM
    period = 'AM' if now.hour < 12 else 'PM'
    header = f"# ğŸ“… {date_str} è´¢ç»åˆ†ææŠ¥å‘Š\n\n> ğŸ“… ç”Ÿæˆæ—¶é—´: {now_str} (åŒ—äº¬æ—¶é—´)\n\n"
    content = header + (markdown_text or '').strip() + '\n'
    report_file = report_dir / f"ğŸ“… {date_str}_{period} è´¢ç»åˆ†ææŠ¥å‘Š_gemini.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(content)
    print_success(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    return report_file


def save_metadata(date_str: str, meta: Dict[str, Any]):
    year_month = date_str[:7]
    report_dir = PROJECT_ROOT / 'docs' / 'archive' / year_month / date_str / 'reports'
    report_dir.mkdir(parents=True, exist_ok=True)
    meta_file = report_dir / 'analysis_meta.json'
    with open(meta_file, 'w', encoding='utf-8') as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)
    print_info(f'å…ƒæ•°æ®å·²ä¿å­˜åˆ°: {meta_file}')


def write_json(path: Path, summary_md: str, articles: List[Dict[str, Any]]):
    data = {
        'summary_markdown': summary_md,
        'articles': articles
    }
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print_success(f'å·²å¯¼å‡º JSON: {path}')


def main():
    args = parse_args()
    start, end = resolve_date_range(args)

    print_header("AI è´¢ç»åˆ†æç³»ç»Ÿ")
    print_info(f"åˆ†ææ—¥æœŸèŒƒå›´: {start} â†’ {end}")
    print_info(f"å­—æ®µé€‰æ‹©æ¨¡å¼: {args.content_field}")
    if args.max_chars > 0:
        print_info(f"å­—ç¬¦æ•°é™åˆ¶: {args.max_chars:,}")
    print()

    # è§£æé…ç½®æ–‡ä»¶ï¼Œä¼˜å…ˆé¡ºåºï¼šconfig.yml > --api-key > ç¯å¢ƒå˜é‡
    config_path = Path(args.config) if args.config else (PROJECT_ROOT / 'config' / 'config.yml')
    api_key: Optional[str] = None
    if config_path.exists():
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                cfg = yaml.safe_load(f) or {}
            # å¸¸è§å±‚çº§å…¼å®¹ï¼šapi_keys.gemini æˆ– gemini.api_key
            api_key = (
                (cfg.get('api_keys') or {}).get('gemini')
                or (cfg.get('gemini') or {}).get('api_key')
            )
            print_success(f'ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼š{config_path}')
        except Exception as e:
            print_warning(f'è¯»å–é…ç½®å¤±è´¥ï¼ˆ{config_path}ï¼‰ï¼š{e}ï¼Œå°†å°è¯•ä½¿ç”¨å‘½ä»¤è¡Œæˆ–ç¯å¢ƒå˜é‡ã€‚')
    else:
        print_warning(f'æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼š{config_path}ï¼Œå°†å°è¯•ä½¿ç”¨å‘½ä»¤è¡Œæˆ–ç¯å¢ƒå˜é‡ã€‚')

    if not api_key:
        api_key = args.api_key or os.getenv('GEMINI_API_KEY')
    if not api_key:
        raise SystemExit("æœªåœ¨é…ç½®/å‚æ•°/ç¯å¢ƒä¸­æ‰¾åˆ° Gemini API Keyã€‚å¯åœ¨ config.yml çš„ api_keys.gemini æˆ– gemini.api_key é…ç½®ï¼Œæˆ–ä½¿ç”¨ --api-key / GEMINI_API_KEYã€‚")

    conn = open_connection(DB_PATH)
    try:
        rows = query_articles(conn, start, end, args.order, args.limit)
    finally:
        conn.close()

    if not rows:
        print_warning('æœªæ‰¾åˆ°æŒ‡å®šæ—¥æœŸèŒƒå›´çš„æ–‡ç« ï¼Œç»ˆæ­¢åˆ†æã€‚')
        return
    print_info(f'å·²è¯»å–æ–‡ç« ï¼š{len(rows):,} æ¡')

    # è¿‡æ»¤æ¥æºä¸å…³é”®è¯ï¼ˆå¯æ§é‡ï¼‰
    selected = rows
    if args.filter_source:
        sources = {s.strip() for s in args.filter_source.split(',') if s.strip()}
        selected = [r for r in selected if (r.get('source') or '') in sources]
    if args.filter_keyword:
        kws = {k.strip() for k in args.filter_keyword.split(',') if k.strip()}
        def match_kw(r: Dict[str, Any]) -> bool:
            text = f"{r.get('title','')} {r.get('summary','')}".lower()
            return any(k.lower() in text for k in kws)
        selected = [r for r in selected if match_kw(r)]
    if args.max_articles and args.max_articles > 0:
        selected = selected[:args.max_articles]

    pairs, total_len = build_corpus(selected, args.max_chars, per_chunk_chars=3000, content_field=args.content_field)
    current_len = sum(len(c) for _, chunks in pairs for c in chunks)
    print_info(f'è¯­æ–™é•¿åº¦: {current_len:,} å­—ç¬¦ï¼ˆåŸå§‹ {total_len:,}ï¼Œé™åˆ¶={args.max_chars:,}ï¼‰')
    if args.max_chars and args.max_chars > 0 and total_len > args.max_chars:
        print_warning(f'è¯­æ–™å·²æŒ‰ä¸Šé™æˆªæ–­ï¼š{total_len:,} â†’ {current_len:,}')

    # æ„å»ºè§„èŒƒåŒ–çš„æ–°é—»æºç»Ÿè®¡ä¿¡æ¯
    stats_info = build_source_stats_block(selected, args.content_field, start, end)

    # ç®€å• RAGï¼šå°†åˆ†å—ä¸²æ¥ä¸€æ¬¡æ€§ç”Ÿæˆï¼ˆå¯å‡çº§ä¸ºå…ˆå¬å›å†ç”Ÿæˆï¼‰
    joined = '\n\n'.join(c for _, chunks in pairs for c in chunks)
    full_content = stats_info + "\n\n" + joined

    try:
        summary_md, usage = call_gemini(api_key, full_content, preferred_model=args.model)
    except Exception as e:
        print_error(f'æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}')
        return

    # ä¿å­˜ Markdown æŠ¥å‘Šï¼ˆæŒ‰ end æ—¥æœŸå‘½åï¼Œæ›´è´´è¿‘æ—¥æŠ¥è¯­ä¹‰ï¼‰
    saved_path = save_markdown(end, summary_md)
    # å…ƒæ•°æ®æŒä¹…åŒ–
    meta = {
        'date_range': {'start': start, 'end': end},
        'articles_used': len(selected),
        'chunks': sum(len(ch) for _, ch in pairs),
        'model_usage': usage,
#         'prompt_file': str((PROJECT_ROOT / 'task' / 'financial_analysis_prompt_pro.md').resolve())
    }
    save_metadata(end, meta)

    # å¯é€‰å¯¼å‡º JSON
    if args.output_json:
        out_path = Path(args.output_json)
        if not out_path.is_absolute():
            out_path = PROJECT_ROOT / out_path
        write_json(out_path, summary_md, rows)

    print_success('åˆ†æå®Œæˆï¼')

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'åˆ†ææ—¥æœŸèŒƒå›´': f"{start} â†’ {end}",
        'å¤„ç†æ–‡ç« æ•°': len(selected),
        'è¯­æ–™å—æ•°': sum(len(ch) for _, ch in pairs),
        'æœ€ç»ˆå­—ç¬¦æ•°': f"{current_len:,}",
        'ä½¿ç”¨æ¨¡å‹': usage.get('model', 'æœªçŸ¥'),
        'Tokenæ¶ˆè€—': f"{usage.get('total_tokens', 0):,}" if usage.get('total_tokens') else 'æœªçŸ¥'
    }
    print_statistics(stats)


if __name__ == '__main__':
    main()


