#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI åˆ†æè„šæœ¬ï¼ˆDeepSeek ç‰ˆæœ¬ï¼ŒåŸºäºæ•°æ®åº“ï¼‰

åŠŸèƒ½ï¼š
- ä» `data/news_data.db` è¯»å–æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„æ–‡ç« 
- è¯­æ–™æ„é€ ä¼˜å…ˆä½¿ç”¨ `content`ï¼ˆæ­£æ–‡ï¼‰ï¼Œä¸ºç©ºåˆ™å›é€€ `summary`
- è°ƒç”¨ DeepSeek æ¨¡å‹ç”Ÿæˆ Markdown åˆ†æ
- å°†æŠ¥å‘Šä¿å­˜åˆ° `docs/archive/YYYY-MM/YYYY-MM-DD/reports/` ä¸‹
- å¯é€‰å¯¼å‡º JSONï¼ˆåŒ…å« summary ä¸æ–‡ç« å…ƒæ•°æ®ï¼‰

ç¤ºä¾‹ï¼š
  - åˆ†æå½“å¤©ï¼š
      python3 scripts/ai_analyze_deepseek.py
  - æŒ‡å®šæ—¥æœŸï¼š
      python3 scripts/ai_analyze_deepseek.py --date 2025-09-29
  - æŒ‡å®šèŒƒå›´å¹¶å¯¼å‡º JSONï¼š
      python3 scripts/ai_analyze_deepseek.py --start 2025-09-28 --end 2025-09-29 --output-json /tmp/analysis.json
"""

import argparse
import json
import os
import sqlite3
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple

import pytz
import yaml

from utils.print_utils import (
    print_header, print_success, print_warning, print_error,
    print_info, print_progress, print_step, print_statistics
)

try:
    from openai import OpenAI
except Exception:
    OpenAI = None  # type: ignore


PROJECT_ROOT = Path(__file__).resolve().parents[1]
DB_PATH = PROJECT_ROOT / 'data' / 'news_data.db'


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description='ä»æ•°æ®åº“è¯»å–æ–°é—»å¹¶è°ƒç”¨ DeepSeek ç”Ÿæˆåˆ†ææŠ¥å‘Š')
    date_group = parser.add_mutually_exclusive_group()
    date_group.add_argument('--date', type=str, help='æŒ‡å®šå•æ—¥ï¼ˆYYYY-MM-DDï¼‰')
    parser.add_argument('--start', type=str, help='å¼€å§‹æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰ï¼Œé»˜è®¤ä¸ºå½“å¤©')
    parser.add_argument('--end', type=str, help='ç»“æŸæ—¥æœŸï¼ˆYYYY-MM-DDï¼‰ï¼Œé»˜è®¤ä¸ºå½“å¤©')
    parser.add_argument('--limit', type=int, default=0, help='æœ€å¤šè¯»å–å¤šå°‘æ¡è®°å½•ï¼ˆ0è¡¨ç¤ºä¸é™åˆ¶ï¼‰')
    parser.add_argument('--max-articles', type=int, help='å¯é€‰ï¼šå¯¹å‚ä¸åˆ†æçš„æ–‡ç« å†æ§é‡ï¼ˆä¼˜å…ˆçº§é«˜äº --limitï¼‰')
    parser.add_argument('--filter-source', type=str, help='ä»…åˆ†ææŒ‡å®šæ¥æºï¼ˆé€—å·åˆ†éš”ï¼‰')
    parser.add_argument('--filter-keyword', type=str, help='ä»…åˆ†ææ ‡é¢˜/æ‘˜è¦åŒ…å«å…³é”®è¯çš„æ–‡ç« ï¼ˆé€—å·åˆ†éš”ï¼ŒORè¯­ä¹‰ï¼‰')
    parser.add_argument('--order', choices=['asc', 'desc'], default='desc', help='æ’åºæ–¹å‘ï¼ŒåŸºäº published ä¼˜å…ˆã€å¦åˆ™ created_at')
    parser.add_argument('--output-json', type=str, help='å¯é€‰ï¼šå°†ç»“æœï¼ˆsummary+æ–‡ç« å…ƒæ•°æ®ï¼‰å¯¼å‡ºä¸º JSON æ–‡ä»¶')
    parser.add_argument('--max-chars', type=int, default=500000, help='ä¼ å…¥æ¨¡å‹çš„æœ€å¤§å­—ç¬¦æ•°ä¸Šé™ï¼Œç”¨äºæ§åˆ¶æˆæœ¬ï¼Œ0 è¡¨ç¤ºä¸é™åˆ¶')
    parser.add_argument('--api-key', type=str, help='å¯é€‰ï¼šæ˜¾å¼ä¼ å…¥ DeepSeek API Keyï¼ˆé»˜è®¤ä»…ä»é…ç½®è¯»å–ï¼‰')
    parser.add_argument('--config', type=str, help='å¯é€‰ï¼šé…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ config/config.ymlï¼‰')
    parser.add_argument('--content-field', choices=['summary', 'content', 'auto'], default='auto', help='é€‰æ‹©åˆ†æå­—æ®µï¼šsummary(æ‘˜è¦ä¼˜å…ˆ)ã€content(æ­£æ–‡ä¼˜å…ˆ)ã€auto(æ™ºèƒ½é€‰æ‹©)')
    parser.add_argument('--model', type=str, default='deepseek-chat', help='DeepSeek æ¨¡å‹åç§°ï¼ˆé»˜è®¤ deepseek-chatï¼‰')
    parser.add_argument('--base-url', type=str, default='https://api.deepseek.com/v3.1_terminus_expires_on_20251015', help='DeepSeek API Base URL')
    return parser.parse_args()


def validate_date(date_str: str) -> str:
    try:
        datetime.strptime(date_str, '%Y-%m-%d')
        return date_str
    except ValueError:
        raise SystemExit(f'æ— æ•ˆæ—¥æœŸæ ¼å¼: {date_str}ï¼Œåº”ä¸º YYYY-MM-DD')


def resolve_date_range(args: argparse.Namespace) -> Tuple[str, str]:
    today = datetime.now(pytz.timezone('Asia/Shanghai')).strftime('%Y-%m-%d')
    if args.date:
        day = validate_date(args.date)
        return day, day
    start = validate_date(args.start) if args.start else today
    end = validate_date(args.end) if args.end else today
    if start > end:
        raise SystemExit(f'å¼€å§‹æ—¥æœŸä¸å¾—æ™šäºç»“æŸæ—¥æœŸ: {start} > {end}')
    return start, end


def open_connection(db_path: Path) -> sqlite3.Connection:
    if not db_path.exists():
        raise SystemExit(f'æ•°æ®åº“ä¸å­˜åœ¨: {db_path}')
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    return conn


def build_query(order: str, limit: int) -> Tuple[str, List[Any]]:
    sql = [
        'SELECT a.id, a.collection_date, a.title, a.link, a.published, a.summary, a.content, s.source_name',
        'FROM news_articles a',
        'JOIN rss_sources s ON a.source_id = s.id',
        'WHERE a.collection_date BETWEEN ? AND ?'
    ]
    params: List[Any] = []

    order_dir = 'DESC' if order.lower() == 'desc' else 'ASC'
    sql.append('ORDER BY COALESCE(a.published, a.created_at) ' + order_dir)

    if limit and limit > 0:
        sql.append('LIMIT ?')
        params.append(limit)

    return '\n'.join(sql), params


def query_articles(conn: sqlite3.Connection, start: str, end: str, order: str, limit: int) -> List[Dict[str, Any]]:
    sql, tail = build_query(order, limit)
    params = [start, end] + tail
    cur = conn.execute(sql, params)
    rows = cur.fetchall()
    results: List[Dict[str, Any]] = []
    for r in rows:
        results.append({
            'id': r['id'],
            'collection_date': r['collection_date'],
            'title': r['title'],
            'link': r['link'],
            'source': r['source_name'],
            'published': r['published'],
            'summary': r['summary'],
            'content': r['content']
        })
    return results


def chunk_text(text: str, max_chars: int = 4000) -> List[str]:
    if not text:
        return []
    if max_chars <= 0:
        return [text]
    chunks: List[str] = []
    start = 0
    n = len(text)
    while start < n:
        end = min(n, start + max_chars)
        boundary = text.rfind('\n\n', start, end)
        if boundary == -1 or boundary <= start + int(max_chars * 0.5):
            boundary = end
        chunks.append(text[start:boundary])
        start = boundary
    return chunks


def build_corpus(articles: List[Dict[str, Any]], max_chars: int, per_chunk_chars: int = 3000, content_field: str = 'auto') -> Tuple[List[Tuple[Dict[str, Any], List[str]]], int]:
    pairs: List[Tuple[Dict[str, Any], List[str]]] = []
    total_len = 0
    for a in articles:
        if content_field == 'summary':
            body = a.get('summary') or a.get('content') or ''
        elif content_field == 'content':
            body = a.get('content') or a.get('summary') or ''
        else:
            summary = a.get('summary', '')
            content = a.get('content', '')
            if len(content) > 5000 and summary:
                body = summary
            else:
                body = content or summary or ''
        title = a.get('title') or ''
        source = a.get('source') or ''
        published = a.get('published') or ''
        link = a.get('link') or ''
        header = f"ã€{title}ã€‘\næ¥æº: {source} | æ—¶é—´: {published}\né“¾æ¥: {link}\n"
        text = header + body
        total_len += len(text)
        chunks = chunk_text(text, per_chunk_chars)
        pairs.append((a, chunks))

    if max_chars and max_chars > 0:
        acc = 0
        trimmed: List[Tuple[Dict[str, Any], List[str]]] = []
        for meta, chunks in pairs:
            kept: List[str] = []
            for c in chunks:
                if acc + len(c) <= max_chars:
                    kept.append(c)
                    acc += len(c)
                else:
                    break
            if kept:
                trimmed.append((meta, kept))
            if acc >= max_chars:
                break
        return trimmed, total_len
    return pairs, total_len


def call_deepseek(api_key: str, base_url: str, model_name: str, content: str) -> Tuple[str, Dict[str, Any]]:
    if OpenAI is None:
        raise SystemExit('æœªå®‰è£… openaiï¼Œè¯·å…ˆå®‰è£…æˆ–åœ¨ç¯å¢ƒä¸­æä¾›ã€‚')

    print_progress(f'æ­£åœ¨ç”ŸæˆæŠ¥å‘Šï¼ˆè¾“å…¥é•¿åº¦ {len(content):,} å­—ç¬¦ï¼‰')

    prompt_path = PROJECT_ROOT / 'task' / 'financial_analysis_prompt_pro.md'
    if not prompt_path.exists():
        raise SystemExit(f'æç¤ºè¯æ–‡ä»¶ä¸å­˜åœ¨: {prompt_path}')
    with open(prompt_path, 'r', encoding='utf-8') as f:
        system_prompt = f.read()

    client = OpenAI(api_key=api_key, base_url=base_url)

    try:
        print_step(1, 1, f'è°ƒç”¨æ¨¡å‹: {model_name}')
        resp = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": content},
            ],
            stream=False
        )
        print_success(f'æ¨¡å‹è°ƒç”¨æˆåŠŸ: {model_name}')
        usage = {}
        try:
            # OpenAI SDK usage ç»“æ„å¯èƒ½ä¸åŒï¼Œè¿™é‡Œåšå®¹é”™è®¿é—®
            usage = {
                'model': getattr(resp, 'model', model_name),
                'prompt_tokens': getattr(getattr(resp, 'usage', {}), 'prompt_tokens', None) or (resp.usage.get('prompt_tokens') if isinstance(resp.usage, dict) else None),
                'completion_tokens': getattr(getattr(resp, 'usage', {}), 'completion_tokens', None) or (resp.usage.get('completion_tokens') if isinstance(resp.usage, dict) else None),
                'total_tokens': getattr(getattr(resp, 'usage', {}), 'total_tokens', None) or (resp.usage.get('total_tokens') if isinstance(resp.usage, dict) else None),
            }
        except Exception:
            pass
        text = resp.choices[0].message.content if resp and resp.choices else ''
        return text, usage
    except Exception as e:
        raise RuntimeError(f'DeepSeek æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼š{e}')


def save_markdown(date_str: str, markdown_text: str) -> Path:
    year_month = date_str[:7]
    report_dir = PROJECT_ROOT / 'docs' / 'archive' / year_month / date_str / 'reports'
    report_dir.mkdir(parents=True, exist_ok=True)
    now_str = datetime.now(pytz.timezone('Asia/Shanghai')).strftime('%Y-%m-%d %H:%M:%S')
    header = f"# ğŸ“… {date_str} è´¢ç»åˆ†ææŠ¥å‘Š\n\n> ğŸ“… ç”Ÿæˆæ—¶é—´: {now_str} (åŒ—äº¬æ—¶é—´)\n\n"
    content = header + (markdown_text or '').strip() + '\n'
    report_file = report_dir / f"ğŸ“… {date_str} è´¢ç»åˆ†ææŠ¥å‘Š.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(content)
    print_success(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    return report_file


def save_metadata(date_str: str, meta: Dict[str, Any]):
    year_month = date_str[:7]
    report_dir = PROJECT_ROOT / 'docs' / 'archive' / year_month / date_str / 'reports'
    report_dir.mkdir(parents=True, exist_ok=True)
    meta_file = report_dir / 'analysis_meta.json'
    with open(meta_file, 'w', encoding='utf-8') as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)
    print_info(f'å…ƒæ•°æ®å·²ä¿å­˜åˆ°: {meta_file}')


def write_json(path: Path, summary_md: str, articles: List[Dict[str, Any]]):
    data = {
        'summary_markdown': summary_md,
        'articles': articles
    }
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print_success(f'å·²å¯¼å‡º JSON: {path}')


def main():
    args = parse_args()
    start, end = resolve_date_range(args)

    print_header("AI è´¢ç»åˆ†æç³»ç»Ÿï¼ˆDeepSeekï¼‰")
    print_info(f"åˆ†ææ—¥æœŸèŒƒå›´: {start} â†’ {end}")
    print_info(f"å­—æ®µé€‰æ‹©æ¨¡å¼: {args.content_field}")
    if args.max_chars > 0:
        print_info(f"å­—ç¬¦æ•°é™åˆ¶: {args.max_chars:,}")
    print()

    # è§£æé…ç½®æ–‡ä»¶ï¼Œä¼˜å…ˆé¡ºåºï¼šconfig.yml > --api-keyï¼ˆä¸å†ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼‰
    config_path = Path(args.config) if args.config else (PROJECT_ROOT / 'config' / 'config.yml')
    api_key: Optional[str] = None
    if config_path.exists():
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                cfg = yaml.safe_load(f) or {}
            # æ”¯æŒ api_keys.deepseek æˆ– deepseek.api_key
            api_key = (
                (cfg.get('api_keys') or {}).get('deepseek')
                or (cfg.get('deepseek') or {}).get('api_key')
            )
            if api_key:
                print_success(f'ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼š{config_path}')
        except Exception as e:
            print_warning(f'è¯»å–é…ç½®å¤±è´¥ï¼ˆ{config_path}ï¼‰ï¼š{e}ï¼Œå°†å°è¯•ä½¿ç”¨å‘½ä»¤è¡Œæˆ–ç¯å¢ƒå˜é‡ã€‚')
    else:
        print_warning(f'æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼š{config_path}ï¼Œå°†å°è¯•ä½¿ç”¨å‘½ä»¤è¡Œæˆ–ç¯å¢ƒå˜é‡ã€‚')

    if not api_key:
        api_key = args.api_key  # ä»…å…è®¸å‘½ä»¤è¡Œè¦†ç›–ï¼Œä¸å†ä»ç¯å¢ƒå˜é‡è¯»å–
    if not api_key:
        raise SystemExit("æœªåœ¨é…ç½®æˆ–å‘½ä»¤è¡Œå‚æ•°ä¸­æ‰¾åˆ° DeepSeek API Keyã€‚è¯·åœ¨ config.yml çš„ api_keys.deepseek æˆ– deepseek.api_key é…ç½®ï¼Œæˆ–ä½¿ç”¨ --api-keyã€‚")

    conn = open_connection(DB_PATH)
    try:
        rows = query_articles(conn, start, end, args.order, args.limit)
    finally:
        conn.close()

    if not rows:
        print_warning('æœªæ‰¾åˆ°æŒ‡å®šæ—¥æœŸèŒƒå›´çš„æ–‡ç« ï¼Œç»ˆæ­¢åˆ†æã€‚')
        return
    print_info(f'å·²è¯»å–æ–‡ç« ï¼š{len(rows):,} æ¡')

    selected = rows
    if args.filter_source:
        sources = {s.strip() for s in args.filter_source.split(',') if s.strip()}
        selected = [r for r in selected if (r.get('source') or '') in sources]
    if args.filter_keyword:
        kws = {k.strip() for k in args.filter_keyword.split(',') if k.strip()}
        def match_kw(r: Dict[str, Any]) -> bool:
            text = f"{r.get('title','')} {r.get('summary','')}".lower()
            return any(k.lower() in text for k in kws)
        selected = [r for r in selected if match_kw(r)]
    if args.max_articles and args.max_articles > 0:
        selected = selected[:args.max_articles]

    pairs, total_len = build_corpus(selected, args.max_chars, per_chunk_chars=3000, content_field=args.content_field)
    current_len = sum(len(c) for _, chunks in pairs for c in chunks)
    print_info(f'è¯­æ–™é•¿åº¦: {current_len:,} å­—ç¬¦ï¼ˆåŸå§‹ {total_len:,}ï¼Œé™åˆ¶={args.max_chars:,}ï¼‰')
    if args.max_chars and args.max_chars > 0 and total_len > args.max_chars:
        print_warning(f'è¯­æ–™å·²æŒ‰ä¸Šé™æˆªæ–­ï¼š{total_len:,} â†’ {current_len:,}')

    source_stats = {}
    for article in selected:
        source = article.get('source', 'æœªçŸ¥æ¥æº')
        source_stats[source] = source_stats.get(source, 0) + 1

    total_articles = len(selected)
    content_articles = sum(1 for a in selected if a.get('content'))
    content_ratio = (content_articles / total_articles * 100) if total_articles > 0 else 0

    stats_info = f"""
=== æ•°æ®ç»Ÿè®¡ä¿¡æ¯ ===
åˆ†ææ—¥æœŸèŒƒå›´: {start} è‡³ {end}
å¤„ç†æ–‡ç« æ€»æ•°: {total_articles}ç¯‡
å†…å®¹ç±»å‹: {args.content_field}
æ•°æ®å®Œæ•´æ€§: {content_ratio:.1f}%çš„æ–‡ç« åŒ…å«å®Œæ•´å†…å®¹

æ–°é—»æºç»Ÿè®¡:
"""
    for source, count in sorted(source_stats.items()):
        stats_info += f"- {source}: {count}ç¯‡\n"

    stats_info += f"\næ€»è®¡: {total_articles}ç¯‡æ–°é—»æ–‡ç« \n"

    joined = '\n\n'.join(c for _, chunks in pairs for c in chunks)
    full_content = stats_info + "\n\n" + joined

    try:
        summary_md, usage = call_deepseek(api_key, args.base_url, args.model, full_content)
    except Exception as e:
        print_error(f'æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}')
        return

    saved_path = save_markdown(end, summary_md)
    meta = {
        'date_range': {'start': start, 'end': end},
        'articles_used': len(selected),
        'chunks': sum(len(ch) for _, ch in pairs),
        'model_usage': usage,
    }
    save_metadata(end, meta)

    if args.output_json:
        out_path = Path(args.output_json)
        if not out_path.is_absolute():
            out_path = PROJECT_ROOT / out_path
        write_json(out_path, summary_md, rows)

    print_success('åˆ†æå®Œæˆï¼')

    stats = {
        'åˆ†ææ—¥æœŸèŒƒå›´': f"{start} â†’ {end}",
        'å¤„ç†æ–‡ç« æ•°': len(selected),
        'è¯­æ–™å—æ•°': sum(len(ch) for _, ch in pairs),
        'æœ€ç»ˆå­—ç¬¦æ•°': f"{current_len:,}",
        'ä½¿ç”¨æ¨¡å‹': usage.get('model', args.model),
        'Tokenæ¶ˆè€—': f"{usage.get('total_tokens', 0):,}" if usage.get('total_tokens') else 'æœªçŸ¥'
    }
    print_statistics(stats)


if __name__ == '__main__':
    main()


