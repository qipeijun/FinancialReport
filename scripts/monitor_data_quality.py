#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®è´¨é‡ç›‘æ§å·¥å…·

åŠŸèƒ½ï¼š
- åˆ†ææ•°æ®å®Œæ•´æ€§
- æ£€æµ‹é‡å¤æ•°æ®
- ç»Ÿè®¡æ¥æºè¦†ç›–ç‡
- ç›‘æ§æ•°æ®è´¨é‡æŒ‡æ ‡
- ç”Ÿæˆè´¨é‡æŠ¥å‘Š
"""

import argparse
import sqlite3
from datetime import datetime, timedelta
from pathlib import Path
from dataclasses import dataclass
from typing import Dict, List, Any, Optional
import json

from utils.print_utils import (
    print_header, print_success, print_warning, print_error,
    print_info, print_statistics, print_table_header, print_table_row
)
from utils.logger import get_logger
from utils.config_manager import get_db_path

logger = get_logger('data_quality')


@dataclass
class DataQualityReport:
    """æ•°æ®è´¨é‡æŠ¥å‘Š"""
    # åŸºæœ¬ç»Ÿè®¡
    total_articles: int
    date_range: tuple[str, str]
    
    # å†…å®¹å®Œæ•´æ€§
    articles_with_content: int
    articles_with_summary: int
    empty_title_count: int
    empty_link_count: int
    
    # æ•°æ®è´¨é‡
    duplicate_count: int
    avg_title_length: float
    avg_summary_length: float
    avg_content_length: float
    
    # æ¥æºç»Ÿè®¡
    sources_coverage: Dict[str, int]
    total_sources: int
    
    # æ—¶é—´åˆ†å¸ƒ
    daily_distribution: Dict[str, int]
    
    @property
    def content_coverage(self) -> float:
        """å†…å®¹è¦†ç›–ç‡"""
        if self.total_articles == 0:
            return 0.0
        return (self.articles_with_content / self.total_articles) * 100
    
    @property
    def summary_coverage(self) -> float:
        """æ‘˜è¦è¦†ç›–ç‡"""
        if self.total_articles == 0:
            return 0.0
        return (self.articles_with_summary / self.total_articles) * 100
    
    @property
    def quality_score(self) -> float:
        """
        æ•°æ®è´¨é‡è¯„åˆ†ï¼ˆ0-100ï¼‰
        
        è®¡ç®—å…¬å¼ï¼š
        - å†…å®¹å®Œæ•´æ€§ 40%
        - æ‘˜è¦å®Œæ•´æ€§ 30%
        - æ— é‡å¤ç‡ 20%
        - å¿…å¡«å­—æ®µå®Œæ•´æ€§ 10%
        """
        if self.total_articles == 0:
            return 0.0
        
        # å†…å®¹å®Œæ•´æ€§å¾—åˆ†
        content_score = self.content_coverage * 0.4
        
        # æ‘˜è¦å®Œæ•´æ€§å¾—åˆ†
        summary_score = self.summary_coverage * 0.3
        
        # æ— é‡å¤ç‡å¾—åˆ†
        duplicate_rate = (self.duplicate_count / self.total_articles) * 100
        no_duplicate_score = max(0, 100 - duplicate_rate) * 0.2
        
        # å¿…å¡«å­—æ®µå®Œæ•´æ€§å¾—åˆ†
        empty_required = self.empty_title_count + self.empty_link_count
        required_rate = max(0, 100 - (empty_required / self.total_articles * 100))
        required_score = required_rate * 0.1
        
        return content_score + summary_score + no_duplicate_score + required_score
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'basic': {
                'total_articles': self.total_articles,
                'date_range': {
                    'start': self.date_range[0],
                    'end': self.date_range[1]
                }
            },
            'completeness': {
                'content_coverage': f'{self.content_coverage:.1f}%',
                'summary_coverage': f'{self.summary_coverage:.1f}%',
                'articles_with_content': self.articles_with_content,
                'articles_with_summary': self.articles_with_summary,
                'empty_title_count': self.empty_title_count,
                'empty_link_count': self.empty_link_count
            },
            'quality': {
                'quality_score': f'{self.quality_score:.1f}',
                'duplicate_count': self.duplicate_count,
                'avg_title_length': f'{self.avg_title_length:.1f}',
                'avg_summary_length': f'{self.avg_summary_length:.1f}',
                'avg_content_length': f'{self.avg_content_length:.1f}'
            },
            'sources': {
                'total_sources': self.total_sources,
                'coverage': self.sources_coverage
            },
            'distribution': {
                'daily': self.daily_distribution
            }
        }


def analyze_data_quality(db_path: Path, days: int = 7, 
                         start_date: Optional[str] = None,
                         end_date: Optional[str] = None) -> DataQualityReport:
    """
    åˆ†ææ•°æ®è´¨é‡
    
    Args:
        db_path: æ•°æ®åº“è·¯å¾„
        days: åˆ†ææœ€è¿‘å¤šå°‘å¤©ï¼ˆå¦‚æœæœªæŒ‡å®šæ—¥æœŸèŒƒå›´ï¼‰
        start_date: å¼€å§‹æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰
        end_date: ç»“æŸæ—¥æœŸï¼ˆYYYY-MM-DDï¼‰
    
    Returns:
        DataQualityReport: è´¨é‡æŠ¥å‘Š
    """
    logger.info(f"å¼€å§‹åˆ†ææ•°æ®è´¨é‡ï¼Œæ•°æ®åº“: {db_path}")
    
    if not db_path.exists():
        raise FileNotFoundError(f'æ•°æ®åº“ä¸å­˜åœ¨: {db_path}')
    
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    
    # ç¡®å®šæ—¥æœŸèŒƒå›´
    if start_date and end_date:
        date_start = start_date
        date_end = end_date
    else:
        date_start = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
        date_end = datetime.now().strftime('%Y-%m-%d')
    
    logger.info(f"åˆ†ææ—¥æœŸèŒƒå›´: {date_start} è‡³ {date_end}")
    
    # 1. æ€»æ–‡ç« æ•°
    total = conn.execute(
        "SELECT COUNT(*) as cnt FROM news_articles WHERE collection_date BETWEEN ? AND ?",
        (date_start, date_end)
    ).fetchone()['cnt']
    
    logger.debug(f"æ€»æ–‡ç« æ•°: {total}")
    
    # 2. å†…å®¹å®Œæ•´æ€§
    with_content = conn.execute(
        """SELECT COUNT(*) as cnt FROM news_articles 
           WHERE collection_date BETWEEN ? AND ? 
           AND content IS NOT NULL AND content != ''""",
        (date_start, date_end)
    ).fetchone()['cnt']
    
    with_summary = conn.execute(
        """SELECT COUNT(*) as cnt FROM news_articles 
           WHERE collection_date BETWEEN ? AND ? 
           AND summary IS NOT NULL AND summary != ''""",
        (date_start, date_end)
    ).fetchone()['cnt']
    
    empty_title = conn.execute(
        """SELECT COUNT(*) as cnt FROM news_articles 
           WHERE collection_date BETWEEN ? AND ? 
           AND (title IS NULL OR title = '')""",
        (date_start, date_end)
    ).fetchone()['cnt']
    
    empty_link = conn.execute(
        """SELECT COUNT(*) as cnt FROM news_articles 
           WHERE collection_date BETWEEN ? AND ? 
           AND (link IS NULL OR link = '')""",
        (date_start, date_end)
    ).fetchone()['cnt']
    
    # 3. é‡å¤æ£€æµ‹ï¼ˆåŸºäºæ ‡é¢˜å®Œå…¨åŒ¹é…ï¼‰
    duplicates = conn.execute(
        """SELECT COUNT(*) - COUNT(DISTINCT title) as dup FROM news_articles 
           WHERE collection_date BETWEEN ? AND ?""",
        (date_start, date_end)
    ).fetchone()['dup']
    
    # 4. å¹³å‡é•¿åº¦ç»Ÿè®¡
    avg_stats = conn.execute(
        """SELECT 
           AVG(LENGTH(title)) as avg_title,
           AVG(LENGTH(summary)) as avg_summary,
           AVG(LENGTH(content)) as avg_content
           FROM news_articles 
           WHERE collection_date BETWEEN ? AND ?""",
        (date_start, date_end)
    ).fetchone()
    
    avg_title = avg_stats['avg_title'] or 0
    avg_summary = avg_stats['avg_summary'] or 0
    avg_content = avg_stats['avg_content'] or 0
    
    # 5. æ¥æºè¦†ç›–ç‡
    sources = conn.execute(
        """SELECT s.source_name, COUNT(a.id) as cnt
           FROM news_articles a
           JOIN rss_sources s ON a.source_id = s.id
           WHERE a.collection_date BETWEEN ? AND ?
           GROUP BY s.source_name
           ORDER BY cnt DESC""",
        (date_start, date_end)
    ).fetchall()
    
    sources_coverage = {row['source_name']: row['cnt'] for row in sources}
    total_sources = len(sources_coverage)
    
    # 6. æ¯æ—¥åˆ†å¸ƒ
    daily_dist = conn.execute(
        """SELECT collection_date, COUNT(*) as cnt
           FROM news_articles
           WHERE collection_date BETWEEN ? AND ?
           GROUP BY collection_date
           ORDER BY collection_date""",
        (date_start, date_end)
    ).fetchall()
    
    daily_distribution = {row['collection_date']: row['cnt'] for row in daily_dist}
    
    conn.close()
    
    report = DataQualityReport(
        total_articles=total,
        date_range=(date_start, date_end),
        articles_with_content=with_content,
        articles_with_summary=with_summary,
        empty_title_count=empty_title,
        empty_link_count=empty_link,
        duplicate_count=duplicates,
        avg_title_length=avg_title,
        avg_summary_length=avg_summary,
        avg_content_length=avg_content,
        sources_coverage=sources_coverage,
        total_sources=total_sources,
        daily_distribution=daily_distribution
    )
    
    logger.info(f"æ•°æ®è´¨é‡åˆ†æå®Œæˆï¼Œè´¨é‡è¯„åˆ†: {report.quality_score:.1f}")
    
    return report


def print_quality_report(report: DataQualityReport):
    """æ‰“å°è´¨é‡æŠ¥å‘Š"""
    print_header("æ•°æ®è´¨é‡ç›‘æ§æŠ¥å‘Š")
    
    # åŸºæœ¬ä¿¡æ¯
    print_info(f"ğŸ“… åˆ†ææ—¶é—´èŒƒå›´: {report.date_range[0]} è‡³ {report.date_range[1]}")
    print_info(f"ğŸ“Š æ–‡ç« æ€»æ•°: {report.total_articles:,} ç¯‡")
    print()
    
    # è´¨é‡è¯„åˆ†
    score = report.quality_score
    if score >= 80:
        print_success(f"âœ¨ æ•°æ®è´¨é‡è¯„åˆ†: {score:.1f}/100 (ä¼˜ç§€)")
    elif score >= 60:
        print_warning(f"âš ï¸  æ•°æ®è´¨é‡è¯„åˆ†: {score:.1f}/100 (è‰¯å¥½)")
    else:
        print_error(f"âŒ æ•°æ®è´¨é‡è¯„åˆ†: {score:.1f}/100 (éœ€æ”¹è¿›)")
    print()
    
    # å®Œæ•´æ€§ç»Ÿè®¡
    stats = {
        'å†…å®¹è¦†ç›–ç‡': f'{report.content_coverage:.1f}%',
        'æ‘˜è¦è¦†ç›–ç‡': f'{report.summary_coverage:.1f}%',
        'æœ‰å†…å®¹æ–‡ç« ': f'{report.articles_with_content:,} ç¯‡',
        'æœ‰æ‘˜è¦æ–‡ç« ': f'{report.articles_with_summary:,} ç¯‡',
        'ç©ºæ ‡é¢˜': f'{report.empty_title_count:,} ç¯‡',
        'ç©ºé“¾æ¥': f'{report.empty_link_count:,} ç¯‡',
        'é‡å¤æ–‡ç« ': f'{report.duplicate_count:,} ç¯‡'
    }
    print_statistics(stats)
    print()
    
    # å¹³å‡é•¿åº¦
    print_info("ğŸ“ å¹³å‡é•¿åº¦ç»Ÿè®¡:")
    length_stats = {
        'å¹³å‡æ ‡é¢˜é•¿åº¦': f'{report.avg_title_length:.0f} å­—ç¬¦',
        'å¹³å‡æ‘˜è¦é•¿åº¦': f'{report.avg_summary_length:.0f} å­—ç¬¦',
        'å¹³å‡å†…å®¹é•¿åº¦': f'{report.avg_content_length:.0f} å­—ç¬¦'
    }
    print_statistics(length_stats)
    print()
    
    # æ¥æºåˆ†å¸ƒ
    print_info(f"ğŸ“° æ¥æºåˆ†å¸ƒ (å…± {report.total_sources} ä¸ªæ¥æº):")
    print_table_header(['æ¥æº', 'æ–‡ç« æ•°', 'å æ¯”'], [30, 15, 15])
    
    total = report.total_articles
    for source, count in sorted(report.sources_coverage.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / total * 100) if total > 0 else 0
        print_table_row([source, f'{count:,}', f'{percentage:.1f}%'], [30, 15, 15])
    print()
    
    # æ¯æ—¥åˆ†å¸ƒ
    if report.daily_distribution:
        print_info("ğŸ“ˆ æ¯æ—¥æ–‡ç« æ•°åˆ†å¸ƒ:")
        print_table_header(['æ—¥æœŸ', 'æ–‡ç« æ•°'], [20, 15])
        for date, count in sorted(report.daily_distribution.items()):
            print_table_row([date, f'{count:,}'], [20, 15])


def export_report(report: DataQualityReport, output_path: Path):
    """å¯¼å‡ºæŠ¥å‘Šä¸ºJSON"""
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(report.to_dict(), f, ensure_ascii=False, indent=2)
    print_success(f"æŠ¥å‘Šå·²å¯¼å‡ºåˆ°: {output_path}")
    logger.info(f"è´¨é‡æŠ¥å‘Šå·²å¯¼å‡º: {output_path}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ•°æ®è´¨é‡ç›‘æ§å·¥å…·')
    parser.add_argument('--days', type=int, default=7, help='åˆ†ææœ€è¿‘å‡ å¤©çš„æ•°æ®ï¼ˆé»˜è®¤7å¤©ï¼‰')
    parser.add_argument('--start', type=str, help='å¼€å§‹æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰')
    parser.add_argument('--end', type=str, help='ç»“æŸæ—¥æœŸï¼ˆYYYY-MM-DDï¼‰')
    parser.add_argument('--output', type=str, help='å¯¼å‡ºJSONæŠ¥å‘Šåˆ°æŒ‡å®šè·¯å¾„')
    parser.add_argument('--db', type=str, help='æ•°æ®åº“è·¯å¾„ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼‰')
    args = parser.parse_args()
    
    # è·å–æ•°æ®åº“è·¯å¾„
    if args.db:
        db_path = Path(args.db)
    else:
        db_path = get_db_path()
    
    # åˆ†ææ•°æ®è´¨é‡
    try:
        report = analyze_data_quality(
            db_path,
            days=args.days,
            start_date=args.start,
            end_date=args.end
        )
        
        # æ‰“å°æŠ¥å‘Š
        print_quality_report(report)
        
        # å¯¼å‡ºæŠ¥å‘Š
        if args.output:
            output_path = Path(args.output)
            export_report(report, output_path)
        
    except Exception as e:
        logger.exception(f"æ•°æ®è´¨é‡åˆ†æå¤±è´¥: {e}")
        print_error(f"åˆ†æå¤±è´¥: {e}")
        return 1
    
    return 0


if __name__ == '__main__':
    exit(main())

