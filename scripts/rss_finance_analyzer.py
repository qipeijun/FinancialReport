#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RSSè´¢ç»æ–°é—»æ•°æ®æ”¶é›†å·¥å…·ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

ä¼˜åŒ–å†…å®¹ï¼š
- âœ… é›†æˆæ—¥å¿—ç³»ç»Ÿ
- âœ… é…ç½®ç®¡ç†é›†ä¸­åŒ–  
- âœ… å¹¶å‘æŠ“å–RSSæº
- âœ… è¿›åº¦æ¡æ˜¾ç¤º
- âœ… æ”¹è¿›çš„é”™è¯¯å¤„ç†
- âœ… æ•°æ®åº“æ‰¹é‡æ“ä½œ
- âœ… æ™ºèƒ½å»é‡

ç”¨æ³•ç¤ºä¾‹ï¼š
  python3 scripts/rss_finance_analyzer_optimized.py --fetch-content
"""

import argparse
import json
import re
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode
import html as html_lib

import feedparser
import requests
from tqdm import tqdm
from bs4 import BeautifulSoup
from readability import Document

from utils.logger import get_logger
from utils.config_manager import get_config
from utils.db_manager import DatabaseManager, retry_on_db_error
from utils.deduplication import deduplicate_items
from utils.print_utils import (
    print_header, print_success, print_warning, print_error,
    print_info, print_statistics
)

logger = get_logger('rss_analyzer')


class RSSAnalyzer:
    """RSSæŠ“å–åˆ†æå™¨"""
    
    def __init__(self, db_path: Path, http_cache_path: Path):
        self.db = DatabaseManager(db_path)
        self.http_cache_path = http_cache_path
        self.http_cache = self._load_http_cache()
    
    def _load_http_cache(self) -> dict:
        """åŠ è½½HTTPç¼“å­˜"""
        if self.http_cache_path.exists():
            try:
                with open(self.http_cache_path, 'r', encoding='utf-8') as f:
                    cache = json.load(f)
                    logger.debug(f"åŠ è½½HTTPç¼“å­˜: {len(cache)} æ¡")
                    return cache
            except Exception as e:
                logger.warning(f"åŠ è½½HTTPç¼“å­˜å¤±è´¥: {e}")
                return {}
        return {}
    
    def _save_http_cache(self):
        """ä¿å­˜HTTPç¼“å­˜"""
        try:
            import builtins
            self.http_cache_path.parent.mkdir(parents=True, exist_ok=True)
            with builtins.open(self.http_cache_path, 'w', encoding='utf-8') as f:
                json.dump(self.http_cache, f, ensure_ascii=False, indent=2)
            logger.debug(f"ä¿å­˜HTTPç¼“å­˜: {len(self.http_cache)} æ¡")
        except Exception as e:
            logger.error(f"ä¿å­˜HTTPç¼“å­˜å¤±è´¥: {e}")
    
    @staticmethod
    def normalize_link(raw_url: str) -> str:
        """è§„èŒƒåŒ–é“¾æ¥"""
        if not raw_url:
            return raw_url
        try:
            parsed = urlparse(raw_url)
            netloc = (parsed.netloc or '').lower()
            tracked_params = {'utm_source', 'utm_medium', 'utm_campaign', 'utm_term', 
                             'utm_content', 'spm', 'from', 'ref', 'ref_src'}
            q = [(k, v) for k, v in parse_qsl(parsed.query, keep_blank_values=True) 
                 if k not in tracked_params]
            query = urlencode(q, doseq=True)
            path = parsed.path.rstrip('/')
            normalized = urlunparse((parsed.scheme, netloc, path, '', query, ''))
            return normalized
        except Exception as e:
            logger.warning(f"é“¾æ¥è§„èŒƒåŒ–å¤±è´¥: {raw_url}, {e}")
            return raw_url
    
    @staticmethod
    def normalize_title(title: str) -> str:
        """æ ‡é¢˜è§„èŒƒåŒ–"""
        if not title:
            return ''
        t = title.strip()
        t = re.sub(r'\s+', ' ', t)
        t = re.sub(r'^[\-\sÂ·ã€\[]+', '', t)
        t = re.sub(r'[\-\sÂ·ã€‘\]]+$', '', t)
        return t
    
    @staticmethod
    def enhance_text_quality(text: str) -> str:
        """å¢å¼ºæ–‡æœ¬æ¸…æ´—"""
        if not text:
            return ''
        cleaned = text
        patterns = [
            r'ç‚¹å‡»(é˜…è¯»|æŸ¥çœ‹).*?(åŸæ–‡|å…¨æ–‡).*',
            r'æœ¬æ–‡(æ¥æº|è½¬è½½).*',
            r'å…è´£å£°æ˜[:ï¼š].*',
            r'è´£ä»»ç¼–è¾‘[:ï¼š].*',
            r'å¾®ä¿¡å…¬ä¼—.*',
            r'ç‰ˆæƒ.*(æ‰€æœ‰|å½’åŸä½œè€…æ‰€æœ‰).*',
        ]
        for p in patterns:
            cleaned = re.sub(p, '', cleaned, flags=re.IGNORECASE)
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        return cleaned
    
    @staticmethod
    def clean_html_to_text(raw_html: str) -> str:
        """HTMLè½¬æ–‡æœ¬"""
        if not raw_html:
            return ''
        raw_html = re.sub(r'<(script|style)[\s\S]*?>[\s\S]*?</\1>', ' ', raw_html, flags=re.IGNORECASE)
        text = re.sub(r'<[^>]+>', ' ', raw_html)
        text = html_lib.unescape(text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    def _extract_with_custom_rules(self, soup: BeautifulSoup, url: str) -> str:
        """ä½¿ç”¨è‡ªå®šä¹‰è§„åˆ™æå–æ­£æ–‡ï¼ˆé’ˆå¯¹ç‰¹å®šç½‘ç«™ï¼‰"""
        domain = urlparse(url).netloc.lower()
        
        # ä¸­æ–°ç½‘è´¢ç»
        if 'chinanews.com' in domain:
            # ä¼˜å…ˆä½¿ç”¨ .left_zwï¼ˆæœ€ç²¾ç¡®çš„æ­£æ–‡å®¹å™¨ï¼‰
            content_div = soup.select_one('.left_zw')
            if content_div:
                # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
                for tag in content_div.select('script, style, .editor, .adEditor, .keywords, .share, .pictext, div.pictext'):
                    tag.decompose()
                
                # åªä¿ç•™pæ ‡ç­¾çš„æ–‡æœ¬ï¼ˆæ­£æ–‡é€šå¸¸åœ¨pæ ‡ç­¾ä¸­ï¼‰
                paragraphs = content_div.find_all('p', recursive=True)
                text_parts = []
                for p in paragraphs:
                    p_text = p.get_text(strip=True)
                    if p_text and len(p_text) > 10:  # å¿½ç•¥å¤ªçŸ­çš„æ®µè½
                        text_parts.append(p_text)
                
                text = ' '.join(text_parts)
                if len(text) > 100:
                    return text
            
            # å¤‡é€‰æ–¹æ¡ˆ
            for selector in ['.content_maincontent_content', '.content', '#content']:
                content_div = soup.select_one(selector)
                if content_div:
                    for tag in content_div.select('script, style, .editor, .keywords, .share'):
                        tag.decompose()
                    text = content_div.get_text(separator=' ', strip=True)
                    if len(text) > 100:
                        return text
        
        # åå°”è¡—è§é—»ï¼ˆéœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œå¯èƒ½æ˜¯åŠ¨æ€åŠ è½½ï¼‰
        elif 'wallstreetcn.com' in domain:
            # åå°”è¡—è§é—»çš„å†…å®¹å¯èƒ½æ˜¯Reactæ¸²æŸ“çš„ï¼Œç›´æ¥æå–å¯è§æ–‡æœ¬
            # å°è¯•ä»summaryæˆ–descriptionä¸­è·å–å†…å®¹
            for selector in ['meta[property="og:description"]', 'meta[name="description"]']:
                meta = soup.select_one(selector)
                if meta and meta.get('content'):
                    text = meta['content'].strip()
                    if len(text) > 100:
                        return text
            
            # å°è¯•å…¶ä»–å¯èƒ½çš„å®¹å™¨
            for selector in ['.article-content', '[class*="content"]', 'article']:
                content_div = soup.select_one(selector)
                if content_div:
                    for tag in content_div.select('script, style, .ad, .advertisement, .related, aside'):
                        tag.decompose()
                    text = content_div.get_text(separator=' ', strip=True)
                    if len(text) > 100:
                        return text
        
        # 36æ°ª
        elif '36kr.com' in domain:
            for selector in ['.articleDetailContent', 'article', '.common-width', '[class*="article"]']:
                content_div = soup.select_one(selector)
                if content_div:
                    for tag in content_div.select('script, style, .ad, aside'):
                        tag.decompose()
                    
                    paragraphs = content_div.find_all(['p', 'div'], recursive=True)
                    text_parts = []
                    for p in paragraphs:
                        p_text = p.get_text(strip=True)
                        if p_text and len(p_text) > 10:
                            text_parts.append(p_text)
                    
                    text = ' '.join(text_parts)
                    if len(text) > 100:
                        return text
        
        # ä¸œæ–¹è´¢å¯Œ
        elif 'eastmoney.com' in domain:
            for selector in ['#ContentBody', '.Body', 'article']:
                content_div = soup.select_one(selector)
                if content_div:
                    for tag in content_div.select('script, style, .ad'):
                        tag.decompose()
                    
                    paragraphs = content_div.find_all('p', recursive=True)
                    text_parts = []
                    for p in paragraphs:
                        p_text = p.get_text(strip=True)
                        if p_text and len(p_text) > 10:
                            text_parts.append(p_text)
                    
                    text = ' '.join(text_parts)
                    if len(text) > 100:
                        return text
        
        # ç¬¬ä¸€è´¢ç»
        elif 'yicai.com' in domain:
            for selector in ['.m-txt', 'article', '.article-content']:
                content_div = soup.select_one(selector)
                if content_div:
                    for tag in content_div.select('script, style, .ad'):
                        tag.decompose()
                    
                    paragraphs = content_div.find_all('p', recursive=True)
                    text_parts = []
                    for p in paragraphs:
                        p_text = p.get_text(strip=True)
                        if p_text and len(p_text) > 10:
                            text_parts.append(p_text)
                    
                    text = ' '.join(text_parts)
                    if len(text) > 100:
                        return text
        
        # æ–°æµªè´¢ç»
        elif 'sina.com' in domain or 'finance.sina.com' in domain:
            for selector in ['#artibody', '.article', 'article']:
                content_div = soup.select_one(selector)
                if content_div:
                    for tag in content_div.select('script, style, .ad, .show_author'):
                        tag.decompose()
                    
                    paragraphs = content_div.find_all('p', recursive=True)
                    text_parts = []
                    for p in paragraphs:
                        p_text = p.get_text(strip=True)
                        if p_text and len(p_text) > 10:
                            text_parts.append(p_text)
                    
                    text = ' '.join(text_parts)
                    if len(text) > 100:
                        return text
        
        # ç™¾åº¦ç™¾å®¶å·
        elif 'baijiahao.baidu.com' in domain:
            for selector in ['.article-content', '#article', '[class*="article"]']:
                content_div = soup.select_one(selector)
                if content_div:
                    for tag in content_div.select('script, style'):
                        tag.decompose()
                    
                    paragraphs = content_div.find_all('p', recursive=True)
                    text_parts = []
                    for p in paragraphs:
                        p_text = p.get_text(strip=True)
                        if p_text and len(p_text) > 10:
                            text_parts.append(p_text)
                    
                    text = ' '.join(text_parts)
                    if len(text) > 100:
                        return text
        
        # è™å—…ç½‘
        elif 'huxiu.com' in domain:
            for selector in ['.article__content', '.article-content-wrap', 'article']:
                content_div = soup.select_one(selector)
                if content_div:
                    for tag in content_div.select('script, style, .ad'):
                        tag.decompose()
                    
                    paragraphs = content_div.find_all(['p', 'div'], recursive=True)
                    text_parts = []
                    for p in paragraphs:
                        p_text = p.get_text(strip=True)
                        if p_text and len(p_text) > 10:
                            text_parts.append(p_text)
                    
                    text = ' '.join(text_parts)
                    if len(text) > 100:
                        return text
        
        # Investing.com
        elif 'investing.com' in domain:
            for selector in ['.article_WYSIWYG__O0uhW', 'article', '[class*="article"]']:
                content_div = soup.select_one(selector)
                if content_div:
                    for tag in content_div.select('script, style'):
                        tag.decompose()
                    
                    paragraphs = content_div.find_all('p', recursive=True)
                    text_parts = []
                    for p in paragraphs:
                        p_text = p.get_text(strip=True)
                        if p_text and len(p_text) > 10:
                            text_parts.append(p_text)
                    
                    text = ' '.join(text_parts)
                    if len(text) > 100:
                        return text
        
        return ''
    
    def fetch_article_content(self, url: str, timeout: int = 10) -> str:
        """æŠ“å–æ–‡ç« æ­£æ–‡ï¼ˆæ™ºèƒ½æå–ï¼‰"""
        try:
            resp = requests.get(url, timeout=timeout, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            })
            resp.raise_for_status()
            
            # å¤„ç†ç¼–ç 
            if resp.encoding and resp.encoding.lower() not in ['utf-8', 'utf8']:
                for encoding in ['utf-8', 'gbk', 'gb2312', 'gb18030', 'latin1']:
                    try:
                        html_content = resp.content.decode(encoding)
                        break
                    except (UnicodeDecodeError, LookupError):
                        continue
                else:
                    html_content = resp.content.decode('utf-8', errors='ignore')
            else:
                html_content = resp.text
            
            # ç­–ç•¥1ï¼šä½¿ç”¨è‡ªå®šä¹‰è§„åˆ™ï¼ˆé’ˆå¯¹ç‰¹å®šç½‘ç«™ï¼‰
            soup = BeautifulSoup(html_content, 'lxml')
            custom_text = self._extract_with_custom_rules(soup, url)
            if custom_text and len(custom_text) > 100:
                logger.debug(f"ä½¿ç”¨è‡ªå®šä¹‰è§„åˆ™æå–æ­£æ–‡: {url}")
                return custom_text
            
            # ç­–ç•¥2ï¼šä½¿ç”¨ readability-lxmlï¼ˆé€šç”¨æ™ºèƒ½æå–ï¼‰
            try:
                doc = Document(html_content)
                article_html = doc.summary()
                
                # è§£ææå–çš„HTML
                article_soup = BeautifulSoup(article_html, 'lxml')
                
                # ç§»é™¤ä¸éœ€è¦çš„æ ‡ç­¾
                for tag in article_soup.select('script, style, iframe, nav, header, footer, aside'):
                    tag.decompose()
                
                # æå–æ–‡æœ¬
                text = article_soup.get_text(separator=' ', strip=True)
                
                # æ¸…ç†å¤šä½™ç©ºç™½
                text = re.sub(r'\s+', ' ', text).strip()
                
                if len(text) > 100:
                    logger.debug(f"ä½¿ç”¨Readabilityæå–æ­£æ–‡: {url}")
                    return text
            except Exception as e:
                logger.debug(f"Readabilityæå–å¤±è´¥ {url}: {e}")
            
            # ç­–ç•¥3ï¼šé€šç”¨è§„åˆ™ï¼ˆä½œä¸ºåå¤‡ï¼‰
            # å°è¯•å¸¸è§çš„æ­£æ–‡å®¹å™¨
            for selector in ['article', '.article', '#article', '.content', '#content', 
                           '.post-content', '.entry-content', 'main']:
                content_div = soup.select_one(selector)
                if content_div:
                    for tag in content_div.select('script, style, nav, header, footer, aside'):
                        tag.decompose()
                    text = content_div.get_text(separator=' ', strip=True)
                    text = re.sub(r'\s+', ' ', text).strip()
                    if len(text) > 100:
                        logger.debug(f"ä½¿ç”¨é€šç”¨è§„åˆ™æå–æ­£æ–‡: {url}")
                        return text
            
            # å¦‚æœæ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥ï¼Œè¿”å›ç©º
            logger.debug(f"æ— æ³•æå–æœ‰æ•ˆæ­£æ–‡: {url}")
            return ''
            
        except Exception as e:
            # é™é»˜å¤±è´¥ï¼Œæ­£æ–‡æŠ“å–å¤±è´¥å¾ˆå¸¸è§ï¼ˆ403/404ç­‰ï¼‰
            logger.debug(f"æ­£æ–‡æŠ“å–å¼‚å¸¸ {url}: {e}")
            return ''
    
    def fetch_rss_feed(self, url: str, source_name: str, limit: int = 5) -> List[Any]:
        """è·å–RSSæºå†…å®¹ï¼ˆæ”¯æŒç¼“å­˜å’Œé‡è¯•ï¼‰"""
        headers = {'User-Agent': 'Mozilla/5.0 (compatible; FinanceBot/1.0)'}
        
        # æ¡ä»¶GET
        cache_entry = self.http_cache.get(url, {})
        if cache_entry.get('etag'):
            headers['If-None-Match'] = cache_entry['etag']
        if cache_entry.get('last_modified'):
            headers['If-Modified-Since'] = cache_entry['last_modified']
        
        last_err = None
        for attempt in range(1, 4):
            try:
                response = requests.get(url, timeout=10, headers=headers)
                if response.status_code == 304:
                    return []
                response.raise_for_status()
                
                # æ›´æ–°ç¼“å­˜
                self.http_cache[url] = {
                    'etag': response.headers.get('ETag'),
                    'last_modified': response.headers.get('Last-Modified')
                }
                
                feed = feedparser.parse(response.content)
                entries = feed.entries[:limit] if len(feed.entries) > limit else feed.entries
                return entries
            except Exception as e:
                last_err = e
                if attempt < 3:
                    wait = min(10, 2 ** (attempt - 1))
                    time.sleep(wait)
        
        # åªè®°å½•åˆ°æ—¥å¿—æ–‡ä»¶ï¼Œä¸åœ¨æ§åˆ¶å°æ˜¾ç¤º
        logger.debug(f"{source_name} æŠ“å–å¤±è´¥: {last_err}")
        return []
    
    def fetch_all_sources_parallel(self, rss_sources: dict, limit: int = 5, 
                                   max_workers: int = 5) -> List[Any]:
        """å¹¶å‘æŠ“å–æ‰€æœ‰RSSæº"""
        all_entries = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_source = {
                executor.submit(self.fetch_rss_feed, url, name, limit): name
                for name, url in rss_sources.items()
            }
            
            # ä½¿ç”¨è¿›åº¦æ¡
            success_count = 0
            fail_count = 0
            
            with tqdm(
                total=len(rss_sources), 
                desc="ğŸ“¡ æŠ“å–RSS",
                bar_format='{desc}: {percentage:3.0f}%|{bar:25}| {n}/{total}',
                ncols=70,
                leave=False,
                dynamic_ncols=False
            ) as pbar:
                for future in as_completed(future_to_source):
                    source_name = future_to_source[future]
                    try:
                        entries = future.result()
                        if entries:
                            for entry in entries:
                                entry.source = source_name
                            all_entries.extend(entries)
                            success_count += 1
                        else:
                            fail_count += 1
                    except Exception:
                        fail_count += 1
                    
                    pbar.update(1)
        
        
        print(f"âœ“ æŠ“å–å®Œæˆ: {success_count}/{len(rss_sources)} ä¸ªæºï¼Œ{len(all_entries)} ç¯‡æ–‡ç« ")
        return all_entries
    
    @retry_on_db_error(max_retries=3)
    def save_to_database(self, entries: List[Any], collection_date: str,
                        rss_sources: dict, fetch_content: bool = False,
                        content_max_length: int = 0) -> int:
        """æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“"""
        if not entries:
            return 0
        
        # åˆå§‹åŒ–æ•°æ®åº“è¡¨
        self._init_database()
        
        # è·å–æˆ–åˆ›å»ºæ¥æºæ˜ å°„
        source_map = self._get_source_map(rss_sources)
        
        # å‡†å¤‡æ–‡ç« æ•°æ®
        article_data = []
        
        for entry in tqdm(
            entries, 
            desc="ğŸ“ å¤„ç†æ•°æ®", 
            ncols=70, 
            bar_format='{desc}: {percentage:3.0f}%|{bar:25}| {n}/{total}',
            leave=False,
            dynamic_ncols=False
        ):
            source_name = getattr(entry, 'source', 'Unknown')
            source_id = source_map.get(source_name)
            
            if source_id is None:
                continue
            
            # å¤„ç†å‘å¸ƒæ—¶é—´
            published = entry.get('published', 'N/A')
            published_parsed = None
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                published_parsed = json.dumps(list(entry.published_parsed))
            
            # æŠ“å–æ­£æ–‡ï¼ˆå¯é€‰ï¼‰
            content_text = ''
            if fetch_content:
                content_text = self.fetch_article_content(entry.get('link', ''))
                if not content_text:
                    content_text = entry.get('summary', 'N/A') or ''
            
            # æˆªæ–­é•¿åº¦
            if content_text and content_max_length > 0:
                content_text = content_text[:content_max_length]
            
            # æ–‡æœ¬å¢å¼º
            summary_text = self.enhance_text_quality(entry.get('summary', 'N/A') or '')
            if content_text:
                content_text = self.enhance_text_quality(content_text)
            
            # è§„èŒƒåŒ–
            norm_title = self.normalize_title(entry.get('title', 'N/A'))
            norm_link = self.normalize_link(entry.get('link', 'N/A'))
            
            article_data.append((
                collection_date,
                norm_title,
                norm_link,
                source_id,
                published,
                published_parsed,
                summary_text,
                content_text if fetch_content else None,
                None  # category
            ))
        
        # æ‰¹é‡æ’å…¥
        sql = '''
            INSERT OR IGNORE INTO news_articles 
            (collection_date, title, link, source_id, published, published_parsed, summary, content, category)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        '''
        
        inserted = self.db.execute_batch(sql, article_data, batch_size=100)
        print(f"âœ“ ä¿å­˜å®Œæˆ: {inserted} ç¯‡æ–°æ–‡ç« å…¥åº“")
        
        return inserted
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„"""
        with self.db.transaction() as conn:
            cursor = conn.cursor()
            
            # åˆ›å»ºæ•°æ®æºè¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS rss_sources (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    source_name TEXT UNIQUE NOT NULL,
                    rss_url TEXT NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # åˆ›å»ºæ–°é—»æ–‡ç« è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS news_articles (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    collection_date TEXT NOT NULL,
                    title TEXT NOT NULL,
                    link TEXT UNIQUE NOT NULL,
                    source_id INTEGER NOT NULL,
                    published TEXT,
                    published_parsed TEXT,
                    summary TEXT,
                    content TEXT,
                    category TEXT,
                    sentiment_score REAL DEFAULT 0,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (source_id) REFERENCES rss_sources (id)
                )
            ''')
            
            # åˆ›å»ºæ ‡ç­¾è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS news_tags (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    article_id INTEGER,
                    tag_type TEXT,
                    tag_value TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (article_id) REFERENCES news_articles (id) ON DELETE CASCADE
                )
            ''')
            
            # åˆ›å»ºç´¢å¼•
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_articles_collection_date ON news_articles(collection_date)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_articles_source ON news_articles(source_id)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_articles_published ON news_articles(published)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_articles_title ON news_articles(title)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_articles_link ON news_articles(link)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_tags_article ON news_tags(article_id)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_tags_value ON news_tags(tag_value)')
            
            # FTS5å…¨æ–‡æ£€ç´¢
            try:
                cursor.execute('''
                    CREATE VIRTUAL TABLE IF NOT EXISTS news_articles_fts USING fts5(
                        title, summary, content, content='news_articles', content_rowid='id'
                    )
                ''')
            except Exception as e:
                logger.debug(f"FTS5ä¸å¯ç”¨: {e}")
        
        logger.debug("æ•°æ®åº“è¡¨ç»“æ„åˆå§‹åŒ–å®Œæˆ")
    
    def _get_source_map(self, rss_sources: dict) -> Dict[str, int]:
        """è·å–æˆ–åˆ›å»ºæ¥æºæ˜ å°„"""
        source_data = [(name, url) for name, url in rss_sources.items()]
        
        with self.db.transaction() as conn:
            cursor = conn.cursor()
            # æ‰¹é‡æ’å…¥
            cursor.executemany(
                "INSERT OR IGNORE INTO rss_sources (source_name, rss_url) VALUES (?, ?)",
                source_data
            )
            
            # è·å–æ˜ å°„
            rows = cursor.execute("SELECT source_name, id FROM rss_sources").fetchall()
            source_map = {row['source_name']: row['id'] for row in rows}
        
        logger.debug(f"æ¥æºæ˜ å°„: {len(source_map)} ä¸ªæ¥æº")
        return source_map
    
    def __del__(self):
        """ææ„æ—¶ä¿å­˜ç¼“å­˜"""
        self._save_http_cache()


def load_rss_sources(config_path: Path) -> dict:
    """ä»é…ç½®æ–‡ä»¶åŠ è½½RSSæº"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        # æ‰å¹³åŒ–åˆ†ç±»ç»“æ„
        rss_sources = {}
        for category, sources in config.items():
            for source_name, url in sources.items():
                rss_sources[source_name] = url
        
        return rss_sources
        
    except FileNotFoundError:
        print_error(f"é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {config_path}")
        return {}
    except Exception as e:
        print_error(f"è¯»å–é…ç½®å¤±è´¥: {e}")
        return {}


def create_directory_structure(base_path: Path):
    """åˆ›å»ºç›®å½•ç»“æ„"""
    subdirs = ['rss_data', 'news_content', 'analysis', 'reports']
    for subdir in subdirs:
        (base_path / subdir).mkdir(parents=True, exist_ok=True)
    logger.debug(f"ç›®å½•ç»“æ„åˆ›å»º: {base_path}")


def export_to_json(entries: List[Any], output_dir: Path, stats: dict):
    """å¯¼å‡ºæ•°æ®åˆ°JSONï¼ˆé™é»˜ï¼‰"""
    try:
        data_file = output_dir / "collected_data.json"
        
        serialized_entries = []
        for entry in entries:
            serialized_entry = {
                'title': entry.get('title', 'N/A'),
                'link': entry.get('link', 'N/A'),
                'published': entry.get('published', 'N/A'),
                'summary': entry.get('summary', 'N/A'),
                'source': getattr(entry, 'source', 'Unknown')
            }
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                serialized_entry['published_parsed'] = list(entry.published_parsed)
            serialized_entries.append(serialized_entry)
        
        data = {
            'collection_date': datetime.now().strftime('%Y-%m-%d'),
            'total_sources': stats.get('total', 0),
            'successful_sources': stats.get('success', 0),
            'failed_sources': stats.get('failed', 0),
            'total_articles': len(serialized_entries),
            'articles': serialized_entries
        }
        
        with open(data_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
    except Exception as e:
        logger.error(f"å¯¼å‡ºJSONå¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='RSSè´¢ç»æ–°é—»æ•°æ®æ”¶é›†å·¥å…·')
    parser.add_argument('--fetch-content', action='store_true', help='æŠ“å–æ­£æ–‡')
    parser.add_argument('--content-max-length', type=int, default=0, help='æ­£æ–‡æœ€å¤§é•¿åº¦')
    parser.add_argument('--only-source', type=str, help='ä»…æŠ“å–æŒ‡å®šæ¥æºï¼ˆé€—å·åˆ†éš”ï¼‰')
    parser.add_argument('--max-workers', type=int, default=5, help='æœ€å¤§å¹¶å‘æ•°')
    parser.add_argument('--deduplicate', action='store_true', help='å¯ç”¨æ™ºèƒ½å»é‡')
    args = parser.parse_args()
    
    print_header("è´¢ç»æ–°é—»æ•°æ®æ”¶é›†ç³»ç»Ÿ")
    
    # è·å–é…ç½®
    config = get_config()
    project_root = config.project_root
    
    # è·å–å½“å‰æ—¥æœŸ
    today = datetime.now().strftime('%Y-%m-%d')
    
    # åˆ›å»ºç›®å½•ç»“æ„
    base_path = project_root / "docs" / "archive" / f"{today[:7]}" / today
    create_directory_structure(base_path)
    
    # æ•°æ®åº“è·¯å¾„
    data_dir = project_root / "data"
    data_dir.mkdir(parents=True, exist_ok=True)
    db_path = data_dir / "news_data.db"
    http_cache_path = data_dir / 'http_cache.json'
    
    # åŠ è½½RSSæº
    rss_config_path = config.get_rss_sources_config()
    rss_sources = load_rss_sources(rss_config_path)
    
    if not rss_sources:
        print_error("æœªèƒ½åŠ è½½RSSæºé…ç½®")
        return 1
    
    print(f"ğŸ“š å·²åŠ è½½ {len(rss_sources)} ä¸ªRSSæº")
    
    # è¿‡æ»¤æ¥æº
    if args.only_source:
        names = {s.strip() for s in args.only_source.split(',') if s.strip()}
        rss_sources = {k: v for k, v in rss_sources.items() if k in names}
        if not rss_sources:
            print_warning('æœªåŒ¹é…åˆ°ä»»ä½•æ¥æº')
            return 1
        print(f"ğŸ” ç­›é€‰åä¿ç•™ {len(rss_sources)} ä¸ªæº")
    
    print()
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = RSSAnalyzer(db_path, http_cache_path)
    
    # å¹¶å‘æŠ“å–
    all_entries = analyzer.fetch_all_sources_parallel(
        rss_sources,
        limit=5,
        max_workers=args.max_workers
    )
    
    if not all_entries:
        print_warning("æœªè·å–åˆ°ä»»ä½•æ–‡ç« ")
        return 0
    
    print()
    
    # æ™ºèƒ½å»é‡ï¼ˆå¯é€‰ï¼‰
    if args.deduplicate:
        before_count = len(all_entries)
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        articles_dict = [
            {
                'title': e.get('title', ''),
                'link': e.get('link', ''),
                'summary': e.get('summary', ''),
                'source': getattr(e, 'source', ''),
                '_original': e
            }
            for e in all_entries
        ]
        
        unique_articles, dedup_stats = deduplicate_items(
            articles_dict,
            threshold=0.85,
            priority_keys=['summary']
        )
        
        # æ¢å¤åŸå§‹æ ¼å¼
        all_entries = [a['_original'] for a in unique_articles]
        
        print(f"âœ“ å»é‡å®Œæˆ: {before_count} â†’ {len(all_entries)} ç¯‡ï¼ˆç§»é™¤ {dedup_stats['removed']} ç¯‡ï¼‰")
        print()
    
    # ä¿å­˜åˆ°æ•°æ®åº“
    inserted = analyzer.save_to_database(
        all_entries,
        today,
        rss_sources,
        fetch_content=args.fetch_content,
        content_max_length=max(0, args.content_max_length)
    )
    
    # å¯¼å‡ºJSON
    export_to_json(all_entries, base_path, {
        'total': len(rss_sources),
        'success': len(rss_sources),
        'failed': 0
    })
    
    # ç»Ÿè®¡ä¿¡æ¯
    print()
    print("=" * 60)
    print(f"  ğŸ“Š é‡‡é›†ç»Ÿè®¡")
    print("=" * 60)
    print(f"  æ—¥æœŸ: {today}")
    print(f"  æ¥æº: {len(rss_sources)} ä¸ªRSSæº")
    print(f"  è·å–: {len(all_entries)} ç¯‡æ–‡ç« ")
    print(f"  å…¥åº“: {inserted} ç¯‡æ–°æ–‡ç« ")
    print(f"  è·¯å¾„: {db_path}")
    print("=" * 60)
    
    return 0


if __name__ == "__main__":
    exit(main())

