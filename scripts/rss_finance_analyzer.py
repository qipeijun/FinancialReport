#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RSSè´¢ç»æ–°é—»æ•°æ®æ”¶é›†å·¥å…·
æŠ“å–å¤šä¸ªè´¢ç»RSSæºï¼Œä¿å­˜åŸå§‹æ•°æ®åˆ°å•ä¸€SQLiteæ•°æ®åº“ä¾›AIåˆ†æ

ç”¨æ³•ç¤ºä¾‹ï¼š
  - ç›´æ¥è¿è¡Œï¼Œæ”¶é›†ä»Šæ—¥æ•°æ®å¹¶å†™å…¥ `data/news_data.db`ï¼ŒåŒæ—¶åœ¨ `docs/archive/YYYY-MM/YYYY-MM-DD/` ä¸‹ç”Ÿæˆæ–‡ä»¶ï¼š
      python3 scripts/rss_finance_analyzer.py

å¯é€‰å‚æ•°ï¼š
  - æŠ“å–æ­£æ–‡å¹¶å…¥åº“ï¼ˆé»˜è®¤ä»…æ‘˜è¦ï¼‰ï¼š
      python3 scripts/rss_finance_analyzer.py --fetch-content [--content-max-length N]
        Â· content-max-length é»˜è®¤ä¸º 0 è¡¨ç¤ºä¸é™åˆ¶ï¼Œä»…å½“ N>0 æ—¶æ‰æˆªæ–­

è¾“å‡ºå†…å®¹ï¼š
  - docs/archive/YYYY-MM/YYYY-MM-DD/rss_data/*.txt   # å„æºRSSæ¡ç›®æ‘˜è¦
  - docs/archive/YYYY-MM/YYYY-MM-DD/news_content/*   # ç®€è¦å†…å®¹æ–‡ä»¶
  - docs/archive/YYYY-MM/YYYY-MM-DD/collected_data.json  # æ±‡æ€»JSON
  - data/news_data.db                                 # ä¸»SQLiteæ•°æ®åº“ï¼ˆæ¨èæŸ¥è¯¢æ¥æºï¼‰

æ•°æ®åº“å…³é”®è¡¨ç»“æ„ï¼ˆå‚è§ init_databaseï¼‰ï¼š
  - rss_sources(id, source_name, rss_url, created_at)
  - news_articles(id, collection_date, title, link[unique], source_id, published, summary, content, created_at, ...)
    Â· å¸¸ç”¨æŸ¥è¯¢æ—¥æœŸå­—æ®µï¼šcollection_date = YYYY-MM-DD
    Â· å¸¸ç”¨è¿æ¥ï¼šnews_articles.source_id -> rss_sources.id

æ³¨æ„ï¼š
  - æŠ“å–æ•°é‡ä¸ºæ¯æºæœ€æ–°è‹¥å¹²æ¡ï¼ˆè§ fetch_rss_feed(limit)ï¼‰ã€‚
  - å¦‚æœå¤šæ¬¡è¿è¡ŒåŒä¸€å¤©ï¼Œæ•°æ®åº“ä¼šå»é‡ `link`ï¼ˆINSERT OR IGNOREï¼‰ã€‚
  - é…åˆ `scripts/query_news_by_date.py` å¯è¿›è¡Œæ—¥æœŸèŒƒå›´/å…³é”®è¯/æ¥æºçš„æŸ¥è¯¢ã€‚
  - è‹¥å¼€å¯ `--fetch-content`ï¼Œå°†å°è¯•æŠ“å–æ–‡ç« æ­£æ–‡å†™å…¥ `content`ï¼Œå¤±è´¥åˆ™å›é€€ä¸º `summary`ã€‚
"""

import os
import sys
import time
import argparse
import requests
import feedparser
from datetime import datetime
from pathlib import Path
import json
import re
from urllib.parse import urlparse
import html as html_lib
import sqlite3

from utils.print_utils import (
    print_header, print_success, print_warning, print_error, 
    print_info, print_progress, print_step, print_statistics
)
def load_http_cache(cache_path: Path) -> dict:
    """åŠ è½½HTTPç¼“å­˜ï¼ˆETag/Last-Modifiedï¼‰ã€‚"""
    if cache_path.exists():
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception:
            return {}
    return {}


def save_http_cache(cache_path: Path, cache: dict):
    """ä¿å­˜HTTPç¼“å­˜ã€‚"""
    try:
        cache_path.parent.mkdir(parents=True, exist_ok=True)
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
    except Exception:
        pass


def normalize_link(raw_url: str) -> str:
    """è§„èŒƒåŒ–é“¾æ¥ï¼šå»é™¤å¸¸è§è¿½è¸ªå‚æ•°ã€ç»Ÿä¸€å¤§å°å†™åŸŸåã€å»é™¤ç‰‡æ®µä¸å°¾éƒ¨æ–œæ ã€‚"""
    if not raw_url:
        return raw_url
    try:
        from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode

        parsed = urlparse(raw_url)
        # å½’ä¸€åŒ–åŸŸåå°å†™
        netloc = (parsed.netloc or '').lower()
        # å»é™¤å¸¸è§è¿½è¸ªå‚æ•°
        tracked_params = {'utm_source', 'utm_medium', 'utm_campaign', 'utm_term', 'utm_content', 'spm', 'from', 'ref', 'ref_src'}
        q = [(k, v) for k, v in parse_qsl(parsed.query, keep_blank_values=True) if k not in tracked_params]
        query = urlencode(q, doseq=True)
        # å»é™¤ç‰‡æ®µä¸å°¾éƒ¨æ–œæ 
        path = parsed.path.rstrip('/')
        normalized = urlunparse((parsed.scheme, netloc, path, '', query, ''))
        return normalized
    except Exception:
        return raw_url


def normalize_title(title: str) -> str:
    """æ ‡é¢˜è§„èŒƒåŒ–ï¼šå»é™¤å¤šä½™ç©ºç™½ä¸å¸¸è§åŒ…è£¹ç¬¦å·ã€‚"""
    if not title:
        return ''
    t = title.strip()
    # åˆå¹¶ç©ºç™½
    t = re.sub(r'\s+', ' ', t)
    # æ¸…ç†å·¦å³åŒ…è£¹ç¬¦å·
    t = re.sub(r'^[\-\sÂ·ã€\[]+', '', t)
    t = re.sub(r'[\-\sÂ·ã€‘\]]+$', '', t)
    return t


def enhance_text_quality(text: str) -> str:
    """å¢å¼ºæ–‡æœ¬æ¸…æ´—ï¼šç§»é™¤æ¨¡æ¿å°¾æ³¨/è¥é”€ç”¨è¯­ç­‰å¸¸è§å™ªéŸ³ã€‚"""
    if not text:
        return ''
    cleaned = text
    patterns = [
        r'ç‚¹å‡»(é˜…è¯»|æŸ¥çœ‹).*?(åŸæ–‡|å…¨æ–‡).*',
        r'æœ¬æ–‡(æ¥æº|è½¬è½½).*',
        r'å…è´£å£°æ˜[:ï¼š].*',
        r'è´£ä»»ç¼–è¾‘[:ï¼š].*',
        r'å¾®ä¿¡å…¬ä¼—.*',
        r'ç‰ˆæƒ.*(æ‰€æœ‰|å½’åŸä½œè€…æ‰€æœ‰).*',
    ]
    for p in patterns:
        cleaned = re.sub(p, '', cleaned, flags=re.IGNORECASE)
    # å†æ¬¡å‹ç¼©ç©ºç™½
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    return cleaned


def load_rss_sources():
    """ä»é…ç½®æ–‡ä»¶åŠ è½½RSSæº"""
    config_path = Path(__file__).parent / "config" / "rss.json"
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        # å°†åˆ†ç±»ç»“æ„æ‰å¹³åŒ–ä¸ºå•ä¸€å­—å…¸
        rss_sources = {}
        for category, sources in config.items():
            for source_name, url in sources.items():
                rss_sources[source_name] = url
        
        print_success(f"ä»é…ç½®æ–‡ä»¶åŠ è½½äº† {len(rss_sources)} ä¸ªRSSæº")
        return rss_sources
        
    except FileNotFoundError:
        print_error(f"é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {config_path}")
        print_info("ä½¿ç”¨é»˜è®¤RSSæºé…ç½®...")
        # å¤‡ç”¨é»˜è®¤é…ç½®
        return {
            "åå°”è¡—è§é—»": "https://dedicated.wallstreetcn.com/rss.xml",
            "36æ°ª": "https://36kr.com/feed",
            "ä¸œæ–¹è´¢å¯Œ": "http://rss.eastmoney.com/rss_partener.xml"
        }
    except Exception as e:
        print_error(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
        return {}


def clean_html_to_text(raw_html: str) -> str:
    """å°†HTMLå†…å®¹ç²—ç•¥æ¸…æ´—ä¸ºçº¯æ–‡æœ¬ï¼ˆæ— å¤–éƒ¨ä¾èµ–ï¼‰ã€‚"""
    if not raw_html:
        return ''
    # å»é™¤è„šæœ¬å’Œæ ·å¼
    raw_html = re.sub(r'<(script|style)[\s\S]*?>[\s\S]*?</\1>', ' ', raw_html, flags=re.IGNORECASE)
    # å»æ ‡ç­¾
    text = re.sub(r'<[^>]+>', ' ', raw_html)
    # HTMLå®ä½“åè½¬ä¹‰
    text = html_lib.unescape(text)
    # å‹ç¼©ç©ºç™½
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def fetch_article_content(url: str, timeout: int = 10) -> str:
    """æŠ“å–æ–‡ç« æ­£æ–‡HTMLå¹¶æ¸…æ´—ä¸ºæ–‡æœ¬ï¼Œå¤±è´¥è¿”å›ç©ºå­—ç¬¦ä¸²ã€‚"""
    try:
        resp = requests.get(url, timeout=timeout, headers={
            'User-Agent': 'Mozilla/5.0 (compatible; FinanceBot/1.0)'
        })
        resp.raise_for_status()

        # æ­£ç¡®å¤„ç†ç¼–ç é—®é¢˜
        if resp.encoding.lower() in ['utf-8', 'utf8']:
            content = resp.text
        else:
            # å°è¯•å¤šç§ç¼–ç æ–¹å¼
            for encoding in ['utf-8', 'gbk', 'gb2312', 'latin1']:
                try:
                    content = resp.content.decode(encoding)
                    break
                except UnicodeDecodeError:
                    continue
            else:
                # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç¼–ç 
                content = resp.content.decode('utf-8', errors='ignore')

        return clean_html_to_text(content)
    except Exception:
        return ''

def create_directory_structure(base_path):
    """åˆ›å»ºç›®å½•ç»“æ„"""
    subdirs = ['rss_data', 'news_content', 'analysis', 'reports']
    for subdir in subdirs:
        (base_path / subdir).mkdir(parents=True, exist_ok=True)
    print_success(f"ç›®å½•ç»“æ„åˆ›å»ºå®Œæˆ: {base_path}")


def init_database(db_path):
    """åˆå§‹åŒ–SQLiteæ•°æ®åº“å¹¶åˆ›å»ºè¡¨ç»“æ„"""
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # åˆ›å»ºæ•°æ®æºè¡¨
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS rss_sources (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            source_name TEXT UNIQUE NOT NULL,
            rss_url TEXT NOT NULL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # åˆ›å»ºæ–°é—»æ–‡ç« è¡¨ï¼ˆæ·»åŠ æ—¥æœŸå­—æ®µï¼Œä¾¿äºæŒ‰æ—¥æœŸæŸ¥è¯¢ï¼‰
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS news_articles (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            collection_date TEXT NOT NULL,  -- æ”¶é›†æ—¥æœŸï¼Œæ ¼å¼: YYYY-MM-DD
            title TEXT NOT NULL,
            link TEXT UNIQUE NOT NULL,
            source_id INTEGER NOT NULL,
            published TEXT,
            published_parsed TEXT,  -- JSONæ ¼å¼å­˜å‚¨parsedæ—¶é—´
            summary TEXT,
            content TEXT,
            category TEXT,
            sentiment_score REAL DEFAULT 0,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (source_id) REFERENCES rss_sources (id)
        )
    ''')
    
    # åˆ›å»ºæ–°é—»æ ‡ç­¾è¡¨ï¼ˆç”¨äºå…³é”®è¯ã€ä¸»é¢˜ç­‰ï¼‰
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS news_tags (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            article_id INTEGER,
            tag_type TEXT,  -- 'keyword', 'category', 'topic', etc.
            tag_value TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (article_id) REFERENCES news_articles (id) ON DELETE CASCADE
        )
    ''')
    
    # åˆ›å»ºç´¢å¼•ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_articles_collection_date ON news_articles(collection_date)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_articles_source ON news_articles(source_id)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_articles_published ON news_articles(published)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_articles_title ON news_articles(title)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_articles_link ON news_articles(link)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_tags_article ON news_tags(article_id)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_tags_value ON news_tags(tag_value)')

    # åˆ›å»º FTS5 è™šè¡¨ï¼ˆè‹¥æ”¯æŒï¼‰ï¼Œç”¨äºå…¨æ–‡æ£€ç´¢ã€‚ä¸ä¸»è¡¨å†…å®¹è”åŠ¨ï¼Œrowid å¯¹åº” news_articles.id
    try:
        cursor.execute('''
            CREATE VIRTUAL TABLE IF NOT EXISTS news_articles_fts USING fts5(
                title, summary, content, content='news_articles', content_rowid='id'
            )
        ''')
    except Exception:
        # æŸäº› SQLite æ„å»ºå¯èƒ½ä¸åŒ…å« FTS5ï¼Œå¿½ç•¥é”™è¯¯
        pass
    
    conn.commit()
    return conn


def fetch_rss_feed(url, source_name, limit=5, cache: dict | None = None):
    """è·å–RSSæºå†…å®¹ï¼ˆæ”¯æŒæ¡ä»¶GETä¸é‡è¯•ï¼‰ã€‚"""
    try:
        print(f"ğŸ” æ­£åœ¨æŠ“å– {source_name} RSS æº...")
        headers = {'User-Agent': 'Mozilla/5.0 (compatible; FinanceBot/1.0)'}
        # æ¡ä»¶ GET
        if cache is not None:
            entry = cache.get(url) or {}
            if entry.get('etag'):
                headers['If-None-Match'] = entry['etag']
            if entry.get('last_modified'):
                headers['If-Modified-Since'] = entry['last_modified']

        last_err = None
        for attempt in range(1, 4):
            try:
                response = requests.get(url, timeout=10, headers=headers)
                if response.status_code == 304:
                    print(f"ğŸŸ¡ æœªä¿®æ”¹ï¼ˆ304ï¼‰ï¼Œä½¿ç”¨ä¸Šæ¬¡æ•°æ®å ä½ï¼š{source_name}")
                    return []
                response.raise_for_status()
                # æ›´æ–°ç¼“å­˜
                if cache is not None:
                    cache[url] = {
                        'etag': response.headers.get('ETag'),
                        'last_modified': response.headers.get('Last-Modified')
                    }
                feed = feedparser.parse(response.content)
                entries = feed.entries[:limit] if len(feed.entries) > limit else feed.entries
                print(f"ğŸ“Š ä» {source_name} è·å–åˆ° {len(entries)} ç¯‡æ–‡ç« ")
                return entries
            except Exception as e:
                last_err = e
                wait = min(10, 2 ** (attempt - 1))
                print(f"âš ï¸ ç¬¬ {attempt} æ¬¡å°è¯•å¤±è´¥ï¼Œ{wait}s åé‡è¯•ï¼š{e}")
                time.sleep(wait)
        print(f"âŒ æŠ“å– {source_name} å¤±è´¥: {str(last_err)}")
        return None
    except Exception as e:
        print(f"âŒ æŠ“å– {source_name} å¤±è´¥: {str(e)}")
        return None


def save_rss_data(entries, source_name, source_url, output_dir):
    """ä¿å­˜RSSæ•°æ®åˆ°æ–‡ä»¶"""
    try:
        # æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦
        safe_name = re.sub(r'[^\w\s-]', '_', source_name)
        file_path = output_dir / f"{safe_name}.txt"
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(f"RSSæº: {source_name}\n")
            f.write(f"URL: {source_url}\n")
            f.write(f"è·å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("-" * 50 + "\n\n")
            
            for i, entry in enumerate(entries, 1):
                f.write(f"æ–‡ç«  {i}:\n")
                f.write(f"æ ‡é¢˜: {entry.get('title', 'N/A')}\n")
                f.write(f"é“¾æ¥: {entry.get('link', 'N/A')}\n")
                f.write(f"å‘å¸ƒæ—¶é—´: {entry.get('published', 'N/A')}\n")
                f.write(f"æ‘˜è¦: {entry.get('summary', 'N/A')}\n")
                f.write("-" * 30 + "\n\n")
        
        print(f"ğŸ’¾ {source_name} RSSæ•°æ®å·²ä¿å­˜åˆ°: {file_path}")
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜ {source_name} RSSæ•°æ®å¤±è´¥: {str(e)}")
        return False


def save_to_database(all_entries, collection_date, db_path, rss_sources, fetch_content: bool = False, content_max_length: int = 5000):
    """ä¿å­˜æ‰€æœ‰æ”¶é›†çš„æ•°æ®åˆ°å•ä¸€SQLiteæ•°æ®åº“"""
    try:
        conn = init_database(db_path)
        cursor = conn.cursor()
        
        # æ’å…¥æˆ–è·å–æ•°æ®æºID
        source_map = {}
        for source_name, source_url in rss_sources.items():
            cursor.execute(
                "INSERT OR IGNORE INTO rss_sources (source_name, rss_url) VALUES (?, ?)",
                (source_name, source_url)
            )
            cursor.execute("SELECT id FROM rss_sources WHERE source_name = ?", (source_name,))
            source_id = cursor.fetchone()[0]
            source_map[source_name] = source_id
        
        # æ’å…¥æ–‡ç« æ•°æ®
        inserted_count = 0
        for entry in all_entries:
            source_name = getattr(entry, 'source', 'Unknown')
            source_id = source_map.get(source_name, None)
            
            # è·³è¿‡æ²¡æœ‰æœ‰æ•ˆæºIDçš„æ¡ç›®
            if source_id is None:
                continue
                
            # å¤„ç†å‘å¸ƒæ—¶é—´
            published = entry.get('published', 'N/A')
            published_parsed = None
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                published_parsed = json.dumps(list(entry.published_parsed))
            
            # æŠ“å–æ­£æ–‡ï¼ˆå¯é€‰ï¼‰
            content_text = ''
            if fetch_content:
                content_text = fetch_article_content(entry.get('link', ''))
                if not content_text:
                    # å›é€€ä¸ºæ‘˜è¦
                    content_text = entry.get('summary', 'N/A') or ''
            # æˆªæ–­é•¿åº¦ï¼ˆä»…å½“æ˜¾å¼ç»™å‡ºæ­£æ•°ä¸Šé™æ—¶ï¼‰
            if content_text and content_max_length and content_max_length > 0:
                content_text = content_text[:content_max_length]

            # æ–‡æœ¬è´¨é‡å¢å¼º
            summary_text = enhance_text_quality(entry.get('summary', 'N/A') or '')
            if content_text:
                content_text = enhance_text_quality(content_text)

            # è§„èŒƒåŒ–æ ‡é¢˜ä¸é“¾æ¥
            norm_title = normalize_title(entry.get('title', 'N/A'))
            norm_link = normalize_link(entry.get('link', 'N/A'))

            # å‡†å¤‡æ–‡ç« æ•°æ®
            article_data = (
                collection_date,  # æ·»åŠ æ”¶é›†æ—¥æœŸå­—æ®µ
                norm_title,
                norm_link,
                source_id,
                published,
                published_parsed,
                summary_text,
                (content_text if fetch_content else None),  # content å­—æ®µ
                None   # category å­—æ®µ
            )
            
            try:
                # é¢å¤–çš„åŸºäºæ ‡é¢˜çš„å»é‡ï¼šåŒæºã€åŒæ—¥ã€åŒæ ‡é¢˜åˆ™è·³è¿‡
                cursor.execute(
                    'SELECT 1 FROM news_articles WHERE collection_date = ? AND source_id = ? AND title = ? LIMIT 1',
                    (collection_date, source_id, norm_title)
                )
                if cursor.fetchone():
                    continue
                cursor.execute('''
                    INSERT OR IGNORE INTO news_articles 
                    (collection_date, title, link, source_id, published, published_parsed, summary, content, category)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', article_data)
                if cursor.rowcount:
                    inserted_count += cursor.rowcount
                    # åŒæ­¥å†™å…¥/æ›´æ–° FTSï¼ˆè‹¥è™šè¡¨å­˜åœ¨ï¼‰
                    try:
                        # è·å–åˆšæ’å…¥çš„ idï¼ˆlink UNIQUE ä¿éšœ row å­˜åœ¨ï¼‰
                        cursor.execute('SELECT id FROM news_articles WHERE link = ?', (norm_link,))
                        row = cursor.fetchone()
                        if row:
                            article_id = row[0]
                            cursor.execute(
                                'INSERT INTO news_articles_fts(rowid, title, summary, content) VALUES (?, ?, ?, ?)',
                                (article_id, norm_title, summary_text, (content_text if fetch_content else ''))
                            )
                    except Exception:
                        # è‹¥ç¯å¢ƒä¸æ”¯æŒ FTS5 æˆ–è™šè¡¨æœªåˆ›å»ºï¼Œè·³è¿‡
                        pass
            except sqlite3.IntegrityError:
                # å¦‚æœé“¾æ¥å·²å­˜åœ¨ï¼Œè·³è¿‡
                continue
        
        conn.commit()
        conn.close()
        
        print(f"âœ… æ•°æ®åº“ä¿å­˜å®Œæˆ: {inserted_count} ç¯‡æ–‡ç« å·²ä¿å­˜åˆ°æ•°æ®åº“: {db_path}")
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥: {str(e)}")
        return False


def export_to_json(all_entries, output_dir, total_sources, successful_sources, failed_sources):
    """å¯¼å‡ºæ•°æ®åˆ°JSONæ–‡ä»¶ï¼ˆå¤‡ç”¨ï¼‰"""
    try:
        data_file = output_dir / "collected_data.json"
        
        # è½¬æ¢æ‰€æœ‰æ•°æ®ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        serialized_entries = []
        for entry in all_entries:
            serialized_entry = {
                'title': entry.get('title', 'N/A'),
                'link': entry.get('link', 'N/A'),
                'published': entry.get('published', 'N/A'),
                'summary': entry.get('summary', 'N/A'),
                'source': getattr(entry, 'source', 'Unknown')
            }
            # æ·»åŠ  published_parsed if it exists
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                serialized_entry['published_parsed'] = list(entry.published_parsed)
            serialized_entries.append(serialized_entry)
        
        data = {
            'collection_date': datetime.now().strftime('%Y-%m-%d'),
            'total_sources': total_sources,
            'successful_sources': successful_sources,
            'failed_sources': failed_sources,
            'total_articles': len(serialized_entries),
            'articles': serialized_entries
        }
        
        with open(data_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… JSONæ•°æ®å·²ä¿å­˜åˆ°: {data_file}")
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜JSONæ•°æ®å¤±è´¥: {str(e)}")
        return False


def main():
    """ä¸»å‡½æ•° - ä»…æ”¶é›†æ•°æ®å¹¶ä¿å­˜åˆ°å•ä¸€SQLiteæ•°æ®åº“"""
    parser = argparse.ArgumentParser(description='RSSè´¢ç»æ–°é—»æ•°æ®æ”¶é›†å·¥å…·')
    parser.add_argument('--fetch-content', action='store_true', help='æŠ“å–æ­£æ–‡å¹¶å†™å…¥æ•°æ®åº“contentå­—æ®µ')
    parser.add_argument('--content-max-length', type=int, default=0, help='æ­£æ–‡æœ€å¤§å­˜å‚¨é•¿åº¦ï¼Œé»˜è®¤0è¡¨ç¤ºä¸é™åˆ¶ï¼Œä»…å½“>0æ—¶æˆªæ–­')
    parser.add_argument('--only-source', type=str, help='ä»…æŠ“å–æŒ‡å®šæ¥æºï¼ˆé€—å·åˆ†éš”ï¼Œä¸é…ç½®æ–‡ä»¶ä¸­çš„åç§°ä¸€è‡´ï¼‰')
    args = parser.parse_args()
    print_header("è´¢ç»æ–°é—»æ•°æ®æ”¶é›†ç³»ç»Ÿ")
    
    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•çš„çˆ¶ç›®å½•ï¼ˆé¡¹ç›®æ ¹ç›®å½•ï¼‰
    script_dir = Path(__file__).parent
    project_root = script_dir.parent
    
    # è·å–å½“å‰æ—¥æœŸå¹¶åˆ›å»ºç›®å½•
    today = datetime.now().strftime('%Y-%m-%d')
    
    # ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹åˆ›å»ºç›®å½•ç»“æ„
    base_path = project_root / "docs" / "archive" / f"{today[:7]}" / today
    create_directory_structure(base_path)
    
    # è®¾ç½®ç›®å½•å’Œä½¿ç”¨å•ä¸€æ•°æ®åº“
    rss_data_dir = base_path / "rss_data"
    news_content_dir = base_path / "news_content"
    analysis_dir = base_path / "analysis"
    reports_dir = base_path / "reports"
    
    # ä½¿ç”¨å•ä¸€ä¸»æ•°æ®åº“ï¼Œå­˜å‚¨åœ¨dataç›®å½•
    data_dir = project_root / "data"
    data_dir.mkdir(parents=True, exist_ok=True)  # åˆ›å»ºdataç›®å½•
    main_db_path = data_dir / "news_data.db"
    http_cache_path = data_dir / 'http_cache.json'
    http_cache = load_http_cache(http_cache_path)
    
    # åŠ è½½RSSæºé…ç½®
    rss_sources = load_rss_sources()
    if not rss_sources:
        print_error("æœªèƒ½åŠ è½½RSSæºé…ç½®ï¼Œç¨‹åºé€€å‡º")
        return
    
    # åˆå§‹åŒ–ç»“æœç»Ÿè®¡
    successful_sources = 0
    failed_sources = []
    all_entries = []
    
    # è·å–æ‰€æœ‰RSSæº
    total_sources = len(rss_sources)
    
    selected_sources = rss_sources
    if args.only_source:
        names = {s.strip() for s in args.only_source.split(',') if s.strip()}
        selected_sources = {k: v for k, v in rss_sources.items() if k in names}
        if not selected_sources:
            print_warning('æœªåŒ¹é…åˆ°ä»»ä½•æ¥æºåç§°ï¼Œé€€å‡ºã€‚')
            return

    for source_name, url in selected_sources.items():
        entries = fetch_rss_feed(url, source_name, cache=http_cache)
        
        if entries:
            # ä¸ºæ¯ä¸ªæ¡ç›®æ·»åŠ æºä¿¡æ¯
            for entry in entries:
                if not hasattr(entry, 'source'):
                    entry.source = source_name
            
            # ä¿å­˜RSSæ•°æ®
            if save_rss_data(entries, source_name, url, rss_data_dir):
                successful_sources += 1
                all_entries.extend(entries)
                
                # ä¿å­˜åˆ°news_contentç›®å½•ï¼ˆä¿å­˜åŸæ–‡å†…å®¹ï¼‰
                for i, entry in enumerate(entries, 1):
                    try:
                        safe_name = re.sub(r'[^\w\s-]', '_', source_name)
                        content_file = news_content_dir / f"{safe_name}_{i}.txt"
                        
                        with open(content_file, 'w', encoding='utf-8') as f:
                            f.write(f"Title: {entry.get('title', 'N/A')}\n")
                            f.write(f"Link: {entry.get('link', 'N/A')}\n")
                            f.write(f"Source: {source_name}\n")
                            f.write(f"Published: {entry.get('published', 'N/A')}\n")
                            f.write(f"Summary: {entry.get('summary', 'N/A')}\n")
                            f.write("-" * 50 + "\n")
                            f.write(f"Full Content: {entry.get('summary', 'N/A')[:2000]}...\n")  # Truncate if too long
                        
                    except Exception as e:
                        print(f"âŒ ä¿å­˜ {source_name} å†…å®¹å¤±è´¥: {str(e)}")
            else:
                failed_sources.append(source_name)
        else:
            failed_sources.append(source_name)
    
    # ä¿å­˜æ‰€æœ‰æ”¶é›†çš„æ•°æ®åˆ°å•ä¸€æ•°æ®åº“
    save_to_database(
        all_entries,
        today,
        main_db_path,
        rss_sources,
        fetch_content=args.fetch_content,
        content_max_length=max(0, args.content_max_length)
    )
    
    # å†™å›HTTPç¼“å­˜
    save_http_cache(http_cache_path, http_cache)

    # åŒæ—¶å¯¼å‡ºJSONä½œä¸ºå¤‡ç”¨ï¼ˆå¯é€‰ï¼‰
    export_to_json(all_entries, base_path, total_sources, successful_sources, failed_sources)
    
    print_success(f"æ•°æ®æ”¶é›†å®Œæˆ: æˆåŠŸå¤„ç† {successful_sources}/{total_sources} ä¸ªRSSæº")
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'æˆåŠŸæºæ•°': f"{successful_sources}/{total_sources}",
        'æ”¶é›†æ–‡ç« æ•°': len(all_entries),
        'å¤±è´¥æºæ•°': len(failed_sources)
    }
    print_statistics(stats)
    
    if failed_sources:
        print_warning(f"ä»¥ä¸‹æºæŠ“å–å¤±è´¥: {', '.join(failed_sources)}")

    print_info("æ•°æ®å·²ä¿å­˜åˆ°:")
    print(f"   - RSSæ•°æ®: {rss_data_dir}")
    print(f"   - æ–°é—»å†…å®¹: {news_content_dir}")
    print(f"   - ä¸»æ•°æ®åº“: {main_db_path}")
    print(f"   - JSONå¤‡ä»½: {base_path}/collected_data.json")


if __name__ == "__main__":
    main()