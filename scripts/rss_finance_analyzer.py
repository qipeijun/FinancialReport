#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RSSè´¢ç»æ–°é—»æ•°æ®æ”¶é›†å·¥å…·
æŠ“å–å¤šä¸ªè´¢ç»RSSæºï¼Œä¿å­˜åŸå§‹æ•°æ®åˆ°å•ä¸€SQLiteæ•°æ®åº“ä¾›AIåˆ†æ

ç”¨æ³•ç¤ºä¾‹ï¼š
  - ç›´æ¥è¿è¡Œï¼Œæ”¶é›†ä»Šæ—¥æ•°æ®å¹¶å†™å…¥ `data/news_data.db`ï¼ŒåŒæ—¶åœ¨ `docs/archive/YYYY-MM/YYYY-MM-DD/` ä¸‹ç”Ÿæˆæ–‡ä»¶ï¼š
      python3 scripts/rss_finance_analyzer.py

è¾“å‡ºå†…å®¹ï¼š
  - docs/archive/YYYY-MM/YYYY-MM-DD/rss_data/*.txt   # å„æºRSSæ¡ç›®æ‘˜è¦
  - docs/archive/YYYY-MM/YYYY-MM-DD/news_content/*   # ç®€è¦å†…å®¹æ–‡ä»¶
  - docs/archive/YYYY-MM/YYYY-MM-DD/collected_data.json  # æ±‡æ€»JSON
  - data/news_data.db                                 # ä¸»SQLiteæ•°æ®åº“ï¼ˆæ¨èæŸ¥è¯¢æ¥æºï¼‰

æ•°æ®åº“å…³é”®è¡¨ç»“æ„ï¼ˆå‚è§ init_databaseï¼‰ï¼š
  - rss_sources(id, source_name, rss_url, created_at)
  - news_articles(id, collection_date, title, link[unique], source_id, published, summary, created_at, ...)
    Â· å¸¸ç”¨æŸ¥è¯¢æ—¥æœŸå­—æ®µï¼šcollection_date = YYYY-MM-DD
    Â· å¸¸ç”¨è¿æ¥ï¼šnews_articles.source_id -> rss_sources.id

æ³¨æ„ï¼š
  - æŠ“å–æ•°é‡ä¸ºæ¯æºæœ€æ–°è‹¥å¹²æ¡ï¼ˆè§ fetch_rss_feed(limit)ï¼‰ã€‚
  - å¦‚æœå¤šæ¬¡è¿è¡ŒåŒä¸€å¤©ï¼Œæ•°æ®åº“ä¼šå»é‡ `link`ï¼ˆINSERT OR IGNOREï¼‰ã€‚
  - é…åˆ `scripts/query_news_by_date.py` å¯è¿›è¡Œæ—¥æœŸèŒƒå›´/å…³é”®è¯/æ¥æºçš„æŸ¥è¯¢ã€‚
"""

import os
import sys
import time
import requests
import feedparser
from datetime import datetime
from pathlib import Path
import json
import re
from urllib.parse import urlparse
import sqlite3

# æ·»åŠ RSSæºåˆ—è¡¨
RSS_SOURCES = {
    "åå°”è¡—è§é—»": "https://dedicated.wallstreetcn.com/rss.xml",
    "36æ°ª": "https://36kr.com/feed",
    "ä¸œæ–¹è´¢å¯Œ": "http://rss.eastmoney.com/rss_partener.xml",
    "ç™¾åº¦è‚¡ç¥¨ç„¦ç‚¹": "http://news.baidu.com/n?cmd=1&class=stock&tn=rss&sub=0",
    "ä¸­æ–°ç½‘": "https://www.chinanews.com.cn/rss/finance.xml",
    "å›½å®¶ç»Ÿè®¡å±€-æœ€æ–°å‘å¸ƒ": "https://www.stats.gov.cn/sj/zxfb/rss.xml",
    "ZeroHedgeåå°”è¡—æ–°é—»": "https://feeds.feedburner.com/zerohedge/feed",
    "ETF Trends": "https://www.etftrends.com/feed/",
    "Federal Reserve Board": "https://www.federalreserve.gov/feeds/press_all.xml",
    "BBCå…¨çƒç»æµ": "http://feeds.bbci.co.uk/news/business/rss.xml",
    "FTä¸­æ–‡ç½‘": "https://www.ftchinese.com/rss/feed",
    "Wall Street Journal": "https://feeds.a.dj.com/rss/RSSWorldNews.xml",
    "Investing.com": "https://www.investing.com/rss/news.rss",
    "Thomson Reuters": "https://ir.thomsonreuters.com/rss/news-releases.xml"
}

def create_directory_structure(base_path):
    """åˆ›å»ºç›®å½•ç»“æ„"""
    subdirs = ['rss_data', 'news_content', 'analysis', 'reports']
    for subdir in subdirs:
        (base_path / subdir).mkdir(parents=True, exist_ok=True)
    print(f"âœ… ç›®å½•ç»“æ„åˆ›å»ºå®Œæˆ: {base_path}")


def init_database(db_path):
    """åˆå§‹åŒ–SQLiteæ•°æ®åº“å¹¶åˆ›å»ºè¡¨ç»“æ„"""
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # åˆ›å»ºæ•°æ®æºè¡¨
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS rss_sources (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            source_name TEXT UNIQUE NOT NULL,
            rss_url TEXT NOT NULL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # åˆ›å»ºæ–°é—»æ–‡ç« è¡¨ï¼ˆæ·»åŠ æ—¥æœŸå­—æ®µï¼Œä¾¿äºæŒ‰æ—¥æœŸæŸ¥è¯¢ï¼‰
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS news_articles (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            collection_date TEXT NOT NULL,  -- æ”¶é›†æ—¥æœŸï¼Œæ ¼å¼: YYYY-MM-DD
            title TEXT NOT NULL,
            link TEXT UNIQUE NOT NULL,
            source_id INTEGER NOT NULL,
            published TEXT,
            published_parsed TEXT,  -- JSONæ ¼å¼å­˜å‚¨parsedæ—¶é—´
            summary TEXT,
            content TEXT,
            category TEXT,
            sentiment_score REAL DEFAULT 0,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (source_id) REFERENCES rss_sources (id)
        )
    ''')
    
    # åˆ›å»ºæ–°é—»æ ‡ç­¾è¡¨ï¼ˆç”¨äºå…³é”®è¯ã€ä¸»é¢˜ç­‰ï¼‰
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS news_tags (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            article_id INTEGER,
            tag_type TEXT,  -- 'keyword', 'category', 'topic', etc.
            tag_value TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (article_id) REFERENCES news_articles (id) ON DELETE CASCADE
        )
    ''')
    
    # åˆ›å»ºç´¢å¼•ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_articles_collection_date ON news_articles(collection_date)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_articles_source ON news_articles(source_id)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_articles_published ON news_articles(published)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_articles_title ON news_articles(title)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_articles_link ON news_articles(link)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_tags_article ON news_tags(article_id)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_tags_value ON news_tags(tag_value)')
    
    conn.commit()
    return conn


def fetch_rss_feed(url, source_name, limit=5):
    """è·å–RSSæºå†…å®¹"""
    try:
        print(f"ğŸ” æ­£åœ¨æŠ“å– {source_name} RSS æº...")
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        
        feed = feedparser.parse(response.content)
        
        # åªå–æœ€æ–°çš„limitç¯‡æ–‡ç« 
        entries = feed.entries[:limit] if len(feed.entries) > limit else feed.entries
        
        print(f"ğŸ“Š ä» {source_name} è·å–åˆ° {len(entries)} ç¯‡æ–‡ç« ")
        return entries
    except Exception as e:
        print(f"âŒ æŠ“å– {source_name} å¤±è´¥: {str(e)}")
        return None


def save_rss_data(entries, source_name, output_dir):
    """ä¿å­˜RSSæ•°æ®åˆ°æ–‡ä»¶"""
    try:
        # æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦
        safe_name = re.sub(r'[^\w\s-]', '_', source_name)
        file_path = output_dir / f"{safe_name}.txt"
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(f"RSSæº: {source_name}\n")
            f.write(f"URL: {RSS_SOURCES[source_name]}\n")
            f.write(f"è·å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("-" * 50 + "\n\n")
            
            for i, entry in enumerate(entries, 1):
                f.write(f"æ–‡ç«  {i}:\n")
                f.write(f"æ ‡é¢˜: {entry.get('title', 'N/A')}\n")
                f.write(f"é“¾æ¥: {entry.get('link', 'N/A')}\n")
                f.write(f"å‘å¸ƒæ—¶é—´: {entry.get('published', 'N/A')}\n")
                f.write(f"æ‘˜è¦: {entry.get('summary', 'N/A')}\n")
                f.write("-" * 30 + "\n\n")
        
        print(f"ğŸ’¾ {source_name} RSSæ•°æ®å·²ä¿å­˜åˆ°: {file_path}")
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜ {source_name} RSSæ•°æ®å¤±è´¥: {str(e)}")
        return False


def save_to_database(all_entries, collection_date, db_path):
    """ä¿å­˜æ‰€æœ‰æ”¶é›†çš„æ•°æ®åˆ°å•ä¸€SQLiteæ•°æ®åº“"""
    try:
        conn = init_database(db_path)
        cursor = conn.cursor()
        
        # æ’å…¥æˆ–è·å–æ•°æ®æºID
        source_map = {}
        for source_name in RSS_SOURCES.keys():
            cursor.execute(
                "INSERT OR IGNORE INTO rss_sources (source_name, rss_url) VALUES (?, ?)",
                (source_name, RSS_SOURCES[source_name])
            )
            cursor.execute("SELECT id FROM rss_sources WHERE source_name = ?", (source_name,))
            source_id = cursor.fetchone()[0]
            source_map[source_name] = source_id
        
        # æ’å…¥æ–‡ç« æ•°æ®
        inserted_count = 0
        for entry in all_entries:
            source_name = getattr(entry, 'source', 'Unknown')
            source_id = source_map.get(source_name, None)
            
            # è·³è¿‡æ²¡æœ‰æœ‰æ•ˆæºIDçš„æ¡ç›®
            if source_id is None:
                continue
                
            # å¤„ç†å‘å¸ƒæ—¶é—´
            published = entry.get('published', 'N/A')
            published_parsed = None
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                published_parsed = json.dumps(list(entry.published_parsed))
            
            # å‡†å¤‡æ–‡ç« æ•°æ®
            article_data = (
                collection_date,  # æ·»åŠ æ”¶é›†æ—¥æœŸå­—æ®µ
                entry.get('title', 'N/A'),
                entry.get('link', 'N/A'),
                source_id,
                published,
                published_parsed,
                entry.get('summary', 'N/A')[:5000],  # é™åˆ¶æ‘˜è¦é•¿åº¦
                None,  # content å­—æ®µ
                None   # category å­—æ®µ
            )
            
            try:
                cursor.execute('''
                    INSERT OR IGNORE INTO news_articles 
                    (collection_date, title, link, source_id, published, published_parsed, summary, content, category)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', article_data)
                inserted_count += cursor.rowcount
            except sqlite3.IntegrityError:
                # å¦‚æœé“¾æ¥å·²å­˜åœ¨ï¼Œè·³è¿‡
                continue
        
        conn.commit()
        conn.close()
        
        print(f"âœ… æ•°æ®åº“ä¿å­˜å®Œæˆ: {inserted_count} ç¯‡æ–‡ç« å·²ä¿å­˜åˆ°æ•°æ®åº“: {db_path}")
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥: {str(e)}")
        return False


def export_to_json(all_entries, output_dir, total_sources, successful_sources, failed_sources):
    """å¯¼å‡ºæ•°æ®åˆ°JSONæ–‡ä»¶ï¼ˆå¤‡ç”¨ï¼‰"""
    try:
        data_file = output_dir / "collected_data.json"
        
        # è½¬æ¢æ‰€æœ‰æ•°æ®ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        serialized_entries = []
        for entry in all_entries:
            serialized_entry = {
                'title': entry.get('title', 'N/A'),
                'link': entry.get('link', 'N/A'),
                'published': entry.get('published', 'N/A'),
                'summary': entry.get('summary', 'N/A'),
                'source': getattr(entry, 'source', 'Unknown')
            }
            # æ·»åŠ  published_parsed if it exists
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                serialized_entry['published_parsed'] = list(entry.published_parsed)
            serialized_entries.append(serialized_entry)
        
        data = {
            'collection_date': datetime.now().strftime('%Y-%m-%d'),
            'total_sources': total_sources,
            'successful_sources': successful_sources,
            'failed_sources': failed_sources,
            'total_articles': len(serialized_entries),
            'articles': serialized_entries
        }
        
        with open(data_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… JSONæ•°æ®å·²ä¿å­˜åˆ°: {data_file}")
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜JSONæ•°æ®å¤±è´¥: {str(e)}")
        return False


def main():
    """ä¸»å‡½æ•° - ä»…æ”¶é›†æ•°æ®å¹¶ä¿å­˜åˆ°å•ä¸€SQLiteæ•°æ®åº“"""
    print("ğŸš€ å¼€å§‹æ‰§è¡Œè´¢ç»æ–°é—»æ•°æ®æ”¶é›†ä»»åŠ¡...")
    
    # è·å–å½“å‰æ—¥æœŸå¹¶åˆ›å»ºç›®å½•
    today = datetime.now().strftime('%Y-%m-%d')
    
    base_path = Path("docs/archive") / f"{today[:7]}" / today
    create_directory_structure(base_path)
    
    # è®¾ç½®ç›®å½•å’Œä½¿ç”¨å•ä¸€æ•°æ®åº“
    rss_data_dir = base_path / "rss_data"
    news_content_dir = base_path / "news_content"
    analysis_dir = base_path / "analysis"
    reports_dir = base_path / "reports"
    
    # ä½¿ç”¨å•ä¸€ä¸»æ•°æ®åº“ï¼Œå­˜å‚¨åœ¨dataç›®å½•
    data_dir = Path("data")
    data_dir.mkdir(parents=True, exist_ok=True)  # åˆ›å»ºdataç›®å½•
    main_db_path = data_dir / "news_data.db"
    
    # åˆå§‹åŒ–ç»“æœç»Ÿè®¡
    successful_sources = 0
    failed_sources = []
    all_entries = []
    
    # è·å–æ‰€æœ‰RSSæº
    total_sources = len(RSS_SOURCES)
    
    for source_name, url in RSS_SOURCES.items():
        entries = fetch_rss_feed(url, source_name)
        
        if entries:
            # ä¸ºæ¯ä¸ªæ¡ç›®æ·»åŠ æºä¿¡æ¯
            for entry in entries:
                if not hasattr(entry, 'source'):
                    entry.source = source_name
            
            # ä¿å­˜RSSæ•°æ®
            if save_rss_data(entries, source_name, rss_data_dir):
                successful_sources += 1
                all_entries.extend(entries)
                
                # ä¿å­˜åˆ°news_contentç›®å½•ï¼ˆä¿å­˜åŸæ–‡å†…å®¹ï¼‰
                for i, entry in enumerate(entries, 1):
                    try:
                        safe_name = re.sub(r'[^\w\s-]', '_', source_name)
                        content_file = news_content_dir / f"{safe_name}_{i}.txt"
                        
                        with open(content_file, 'w', encoding='utf-8') as f:
                            f.write(f"Title: {entry.get('title', 'N/A')}\n")
                            f.write(f"Link: {entry.get('link', 'N/A')}\n")
                            f.write(f"Source: {source_name}\n")
                            f.write(f"Published: {entry.get('published', 'N/A')}\n")
                            f.write(f"Summary: {entry.get('summary', 'N/A')}\n")
                            f.write("-" * 50 + "\n")
                            f.write(f"Full Content: {entry.get('summary', 'N/A')[:2000]}...\n")  # Truncate if too long
                        
                    except Exception as e:
                        print(f"âŒ ä¿å­˜ {source_name} å†…å®¹å¤±è´¥: {str(e)}")
            else:
                failed_sources.append(source_name)
        else:
            failed_sources.append(source_name)
    
    # ä¿å­˜æ‰€æœ‰æ”¶é›†çš„æ•°æ®åˆ°å•ä¸€æ•°æ®åº“
    save_to_database(all_entries, today, main_db_path)
    
    # åŒæ—¶å¯¼å‡ºJSONä½œä¸ºå¤‡ç”¨ï¼ˆå¯é€‰ï¼‰
    export_to_json(all_entries, base_path, total_sources, successful_sources, failed_sources)
    
    print(f"âœ… æ•°æ®æ”¶é›†å®Œæˆ: æˆåŠŸå¤„ç† {successful_sources}/{total_sources} ä¸ªRSSæº")
    print(f"ğŸ“Š æ€»å…±æ”¶é›†åˆ° {len(all_entries)} ç¯‡æ–‡ç« ")
    
    if failed_sources:
        print(f"âš ï¸ ä»¥ä¸‹æºæŠ“å–å¤±è´¥: {', '.join(failed_sources)}")

    print("\nğŸ’¡ æ•°æ®å·²ä¿å­˜åˆ°:")
    print(f"   - RSSæ•°æ®: {rss_data_dir}")
    print(f"   - æ–°é—»å†…å®¹: {news_content_dir}")
    print(f"   - ä¸»æ•°æ®åº“: {main_db_path}")
    print(f"   - JSONå¤‡ä»½: {base_path}/collected_data.json")


if __name__ == "__main__":
    main()