#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI åˆ†æå…¬å…±æ¨¡å—

æå–ai_analyze.pyå’Œai_analyze_deepseek.pyçš„å…¬å…±é€»è¾‘ï¼Œé¿å…ä»£ç é‡å¤
"""

import json
import sqlite3
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple

import pytz

from utils.print_utils import (
    print_success, print_warning, print_info
)


PROJECT_ROOT = Path(__file__).resolve().parents[2]
DB_PATH = PROJECT_ROOT / 'data' / 'news_data.db'


def validate_date(date_str: str) -> str:
    """éªŒè¯æ—¥æœŸæ ¼å¼"""
    try:
        datetime.strptime(date_str, '%Y-%m-%d')
        return date_str
    except ValueError:
        raise SystemExit(f'æ— æ•ˆæ—¥æœŸæ ¼å¼: {date_str}ï¼Œåº”ä¸º YYYY-MM-DD')


def open_connection(db_path: Path) -> sqlite3.Connection:
    """æ‰“å¼€æ•°æ®åº“è¿æ¥"""
    if not db_path.exists():
        raise SystemExit(f'æ•°æ®åº“ä¸å­˜åœ¨: {db_path}')
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    return conn


def build_query(order: str, limit: int) -> Tuple[str, List[Any]]:
    """æ„å»ºSQLæŸ¥è¯¢"""
    sql = [
        'SELECT a.id, a.collection_date, a.title, a.link, a.published, a.summary, a.content, s.source_name',
        'FROM news_articles a',
        'JOIN rss_sources s ON a.source_id = s.id',
        'WHERE a.collection_date BETWEEN ? AND ?'
    ]
    params: List[Any] = []

    order_dir = 'DESC' if order.lower() == 'desc' else 'ASC'
    sql.append('ORDER BY COALESCE(a.published, a.created_at) ' + order_dir)

    if limit and limit > 0:
        sql.append('LIMIT ?')
        params.append(limit)

    return '\n'.join(sql), params


def query_articles(conn: sqlite3.Connection, start: str, end: str, order: str, limit: int) -> List[Dict[str, Any]]:
    """æŸ¥è¯¢æ–‡ç« """
    sql, tail = build_query(order, limit)
    params = [start, end] + tail
    cur = conn.execute(sql, params)
    rows = cur.fetchall()
    results: List[Dict[str, Any]] = []
    for r in rows:
        results.append({
            'id': r['id'],
            'collection_date': r['collection_date'],
            'title': r['title'],
            'link': r['link'],
            'source': r['source_name'],
            'published': r['published'],
            'summary': r['summary'],
            'content': r['content']
        })
    return results


def chunk_text(text: str, max_chars: int = 4000) -> List[str]:
    """æ–‡æœ¬åˆ†å—"""
    if not text:
        return []
    if max_chars <= 0:
        return [text]
    chunks: List[str] = []
    start = 0
    n = len(text)
    while start < n:
        end = min(n, start + max_chars)
        boundary = text.rfind('\n\n', start, end)
        if boundary == -1 or boundary <= start + int(max_chars * 0.5):
            boundary = end
        chunks.append(text[start:boundary])
        start = boundary
    return chunks


def build_corpus(articles: List[Dict[str, Any]], max_chars: int, per_chunk_chars: int = 3000, content_field: str = 'auto') -> Tuple[List[Tuple[Dict[str, Any], List[str]]], int]:
    """æ„é€ åˆ†å—è¯­æ–™"""
    pairs: List[Tuple[Dict[str, Any], List[str]]] = []
    total_len = 0
    for a in articles:
        if content_field == 'summary':
            body = a.get('summary') or a.get('content') or ''
        elif content_field == 'content':
            body = a.get('content') or a.get('summary') or ''
        else:  # auto
            summary = a.get('summary', '')
            content = a.get('content', '')
            if len(content) > 5000 and summary:
                body = summary
            else:
                body = content or summary or ''

        title = a.get('title') or ''
        source = a.get('source') or ''
        published = a.get('published') or ''
        link = a.get('link') or ''
        header = f"ã€{title}ã€‘\næ¥æº: {source} | æ—¶é—´: {published}\né“¾æ¥: {link}\n"
        text = header + body
        total_len += len(text)
        chunks = chunk_text(text, per_chunk_chars)
        pairs.append((a, chunks))

    if max_chars and max_chars > 0:
        acc = 0
        trimmed: List[Tuple[Dict[str, Any], List[str]]] = []
        for meta, chunks in pairs:
            kept: List[str] = []
            for c in chunks:
                if acc + len(c) <= max_chars:
                    kept.append(c)
                    acc += len(c)
                else:
                    break
            if kept:
                trimmed.append((meta, kept))
            if acc >= max_chars:
                break
        return trimmed, total_len
    return pairs, total_len


def _normalize_source_name(name: str) -> str:
    """è§„èŒƒåŒ–æ¥æºåç§°"""
    if not name:
        return 'æœªçŸ¥æ¥æº'
    name = name.strip()
    mapping = {
        'ä¸œæ–¹è´¢å¯Œç½‘': 'ä¸œæ–¹è´¢å¯Œ',
        'å›½å®¶ç»Ÿè®¡å±€-æœ€æ–°å‘å¸ƒ': 'å›½å®¶ç»Ÿè®¡å±€',
        'ä¸­æ–°ç¤¾': 'ä¸­æ–°ç½‘',
        'ä¸­å›½æ–°é—»ç½‘': 'ä¸­æ–°ç½‘',
        'Wall Street CN': 'åå°”è¡—è§é—»',
        'WallstreetCN': 'åå°”è¡—è§é—»',
    }
    return mapping.get(name, name)


def build_source_stats_block(selected: List[Dict[str, Any]], content_field: str, start: str, end: str) -> str:
    """æ„å»ºæ•°æ®ç»Ÿè®¡ä¿¡æ¯å—"""
    tracked = ['åå°”è¡—è§é—»', '36æ°ª', 'ä¸œæ–¹è´¢å¯Œ', 'å›½å®¶ç»Ÿè®¡å±€', 'ä¸­æ–°ç½‘']
    counters: Dict[str, int] = {k: 0 for k in tracked}
    other_count = 0

    for article in selected:
        raw = (article.get('source') or '').strip()
        norm = _normalize_source_name(raw)
        if norm in counters:
            counters[norm] += 1
        else:
            other_count += 1

    total_articles = len(selected)
    content_articles = sum(1 for a in selected if a.get('content'))
    content_ratio = (content_articles / total_articles * 100) if total_articles > 0 else 0

    stats_info = f"""
=== æ•°æ®ç»Ÿè®¡ä¿¡æ¯ ===
åˆ†ææ—¥æœŸèŒƒå›´: {start} è‡³ {end}
å¤„ç†æ–‡ç« æ€»æ•°: {total_articles}ç¯‡
å†…å®¹ç±»å‹: {content_field}
æ•°æ®å®Œæ•´æ€§: {content_ratio:.1f}%çš„æ–‡ç« åŒ…å«å®Œæ•´å†…å®¹

æ–°é—»æºç»Ÿè®¡:
æœ¬æ¬¡åˆ†æåŸºäºä»¥ä¸‹æ–°é—»æºï¼š
"""
    for k in tracked:
        stats_info += f"- {k}ï¼š{counters[k]}ç¯‡\n"
    stats_info += f"- å…¶ä»–æ¥æºï¼š{other_count}ç¯‡\n\n"
    stats_info += f"æ€»è®¡: {total_articles}ç¯‡æ–°é—»æ–‡ç« \n"
    return stats_info


def save_markdown(date_str: str, markdown_text: str, model_suffix: str = 'gemini') -> Path:
    """ä¿å­˜MarkdownæŠ¥å‘Š"""
    year_month = date_str[:7]
    report_dir = PROJECT_ROOT / 'docs' / 'archive' / year_month / date_str / 'reports'
    report_dir.mkdir(parents=True, exist_ok=True)
    now_str = datetime.now(pytz.timezone('Asia/Shanghai')).strftime('%Y-%m-%d %H:%M:%S')
    header = f"# ğŸ“… {date_str} è´¢ç»åˆ†ææŠ¥å‘Š\n\n> ğŸ“… ç”Ÿæˆæ—¶é—´: {now_str} (åŒ—äº¬æ—¶é—´)\n\n"
    content = header + (markdown_text or '').strip() + '\n'
    report_file = report_dir / f"ğŸ“… {date_str} è´¢ç»åˆ†ææŠ¥å‘Š_{model_suffix}.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(content)
    print_success(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    return report_file


def save_metadata(date_str: str, meta: Dict[str, Any]):
    """ä¿å­˜å…ƒæ•°æ®"""
    year_month = date_str[:7]
    report_dir = PROJECT_ROOT / 'docs' / 'archive' / year_month / date_str / 'reports'
    report_dir.mkdir(parents=True, exist_ok=True)
    meta_file = report_dir / 'analysis_meta.json'
    with open(meta_file, 'w', encoding='utf-8') as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)
    print_info(f'å…ƒæ•°æ®å·²ä¿å­˜åˆ°: {meta_file}')


def write_json(path: Path, summary_md: str, articles: List[Dict[str, Any]]):
    """å¯¼å‡ºJSON"""
    data = {
        'summary_markdown': summary_md,
        'articles': articles
    }
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print_success(f'å·²å¯¼å‡º JSON: {path}')


def filter_articles(articles: List[Dict[str, Any]], 
                    filter_source: Optional[str] = None,
                    filter_keyword: Optional[str] = None,
                    max_articles: Optional[int] = None) -> List[Dict[str, Any]]:
    """è¿‡æ»¤æ–‡ç« """
    selected = articles
    
    if filter_source:
        sources = {s.strip() for s in filter_source.split(',') if s.strip()}
        selected = [r for r in selected if (r.get('source') or '') in sources]
    
    if filter_keyword:
        kws = {k.strip() for k in filter_keyword.split(',') if k.strip()}
        def match_kw(r: Dict[str, Any]) -> bool:
            text = f"{r.get('title','')} {r.get('summary','')}".lower()
            return any(k.lower() in text for k in kws)
        selected = [r for r in selected if match_kw(r)]
    
    if max_articles and max_articles > 0:
        selected = selected[:max_articles]
    
    return selected


def resolve_date_range(args) -> Tuple[str, str]:
    """è§£ææ—¥æœŸèŒƒå›´"""
    today = datetime.now(pytz.timezone('Asia/Shanghai')).strftime('%Y-%m-%d')
    if hasattr(args, 'date') and args.date:
        day = validate_date(args.date)
        return day, day
    start = validate_date(args.start) if args.start else today
    end = validate_date(args.end) if args.end else today
    if start > end:
        raise SystemExit(f'å¼€å§‹æ—¥æœŸä¸å¾—æ™šäºç»“æŸæ—¥æœŸ: {start} > {end}')
    return start, end

